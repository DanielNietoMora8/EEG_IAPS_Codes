{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excombatants Classification using wavelets and MKL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#LibrerÃ­as Necesarias Para el CÃ³digo organizadas por tipo de llamado y orden alfabÃ©tico*.\n",
    "import csv\n",
    "#import h5py\n",
    "import scipy\n",
    "import statistics\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "import scipy.io as sio\n",
    "from MKLpy.algorithms import AverageMKL, EasyMKL, KOMD\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import\n",
    "from scipy import signal\n",
    "from scipy import spatial\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn import svm\n",
    "from ckaweightedMKL1 import ckaweightedK\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "#from unipath import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CARGA DE LAS CARACTRERÃSTICAS Y LAS ETIQUETAS BINARIZADAS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.inf) #Imprime todos los valores de los arrays, sirve para visualizar mejor los datos\n",
    "\n",
    "\n",
    "import pickle\n",
    "y = np.ones(150)\n",
    "y[50:100] = y[50:100]*2\n",
    "y[100:150] = y[100:150]*3\n",
    "label_matrix = y\n",
    "\n",
    "num_valences= 3\n",
    "num_rois = 8\n",
    "num_channels= 62\n",
    "num_trials= 50\n",
    "#To assess the performance of the C in each emotion, later to evaluate is not necessary.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Created functions to perform MKL clasiffication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MixKer(K,eta):\n",
    "    Kn= np.zeros_like((K[1]))\n",
    "    for i in range(len(eta)):\n",
    "        Kn= Kn + eta[i]*(K[i])\n",
    "    return(Kn)\n",
    "\n",
    "def KerRBF(X):\n",
    "    d= spatial.distance.pdist(X)\n",
    "    gamma= 1/(X.shape[1]*X.var())\n",
    "    K= -spatial.distance.squareform(d)*gamma\n",
    "    K= np.exp(K)\n",
    "    return(K)\n",
    "    \n",
    "def KerLin(X):\n",
    "    K = np.matmul(X,X.T)\n",
    "    return(K)\n",
    "\n",
    "def graficar(F1_test,Subjects,emocion,save):  \n",
    "    Bandas=np.arange(1,5)\n",
    "    Subject= np.arange(1,Subjects+1)\n",
    "    contador=1\n",
    "   \n",
    "\n",
    "    plt.figure(contador,figsize=(6,5))\n",
    "    plt.title('MKL Emocion: ')\n",
    "    plt.bar(Subject, F1_test[:], align='center', alpha=0.5, color=['black', 'red', 'green', 'blue', 'cyan'])\n",
    "    plt.ylim(0,1)\n",
    "    plt.xlabel('Frequency Band')\n",
    "    plt.ylabel('F1 Score')\n",
    "    if save== 'y':\n",
    "        plt.savefig('FiguresMKL_5/'+'C2_Improved')\n",
    "    plt.show()\n",
    "    contador+=1\n",
    "    \n",
    "    \n",
    "def svc_param_selection(y, nfolds,Kernel_train_Wavelets):\n",
    "    Cs = 1.**np.arange(0,1)\n",
    "    #gammas = [0.001, 0.01, 0.1, 1]\n",
    "    param_grid = {'C':Cs}\n",
    "    clf=svm.SVC(kernel='precomputed',class_weight = 'balanced')\n",
    "    clf.fit(Kernel_train_Wavelets,y)\n",
    "    grid_search = GridSearchCV(clf,param_grid,  cv=nfolds)\n",
    "    grid_search.fit(Kernel_train, y)\n",
    "    grid_search.best_params_\n",
    "    return grid_search.best_params_\n",
    "\n",
    "\n",
    "\n",
    "def Moda(N_Emociones,Subjects,Best_C_Matrix):\n",
    "    Mode_C=np.zeros([N_Emociones])\n",
    "    Mode_aux=np.zeros([Subjects,N_Emociones])\n",
    "    Mode_aux2=np.zeros([Subjects,N_Emociones])\n",
    "    for emo in range(0,N_Emociones):\n",
    "        print('Emocion: ',emo)\n",
    "        for sub in range(Subjects):\n",
    " \n",
    "            a= Best_C_Matrix[sub,emo,:]\n",
    "            a=a.tolist()\n",
    "            Mode_aux[sub,emo]= max(a,key=a.count)           \n",
    "      \n",
    "        try:\n",
    "            c= Mode_aux[:,emo]\n",
    "            c=c.tolist()\n",
    "            Mode_C[emo]=max(c,key=c.count)\n",
    "        except:\n",
    "            print('Nada3')\n",
    "    \n",
    "    print(' Mode Valence: ',Mode_C[0],'\\n Mode Arousal: ',Mode_C[1],'\\n Mode Dominance: ',Mode_C[2], '\\n Mode Liking: ',Mode_C[3])\n",
    "    return Mode_C\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM RBF Multiple Kernel Learning MKL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using 3 valences and 40 bands: \n",
    "### Here it was performed a classification for each valence and it was combined the bands and ROIs in a single dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 150, 8)\n",
      "Analizando Wavelet: sym8\n",
      "(145, 145) (145,)\n",
      "y_test: [1. 1. 1. 1. 1.] SVM: [1. 1. 1. 1. 1.]\n",
      "INICIO TEST\n",
      "0.03333333333333333\n",
      "(145, 145) (145,)\n",
      "y_test: [1. 1. 1. 1. 1.] SVM: [2. 1. 2. 3. 2.]\n",
      "INICIO TEST\n",
      "0.04\n",
      "(145, 145) (145,)\n",
      "y_test: [1. 1. 1. 1. 1.] SVM: [1. 1. 2. 1. 1.]\n",
      "INICIO TEST\n",
      "0.06666666666666667\n",
      "(145, 145) (145,)\n",
      "y_test: [1. 1. 1. 1. 1.] SVM: [1. 1. 1. 1. 1.]\n",
      "INICIO TEST\n",
      "0.1\n",
      "(145, 145) (145,)\n",
      "y_test: [1. 1. 1. 1. 1.] SVM: [1. 1. 1. 1. 1.]\n",
      "INICIO TEST\n",
      "0.13333333333333333\n",
      "(145, 145) (145,)\n",
      "y_test: [1. 1. 1. 1. 1.] SVM: [1. 1. 1. 1. 1.]\n",
      "INICIO TEST\n",
      "0.16666666666666666\n",
      "(145, 145) (145,)\n",
      "y_test: [1. 1. 1. 1. 1.] SVM: [1. 1. 1. 1. 1.]\n",
      "INICIO TEST\n",
      "0.2\n",
      "(145, 145) (145,)\n",
      "y_test: [1. 1. 1. 1. 1.] SVM: [1. 1. 1. 1. 1.]\n",
      "INICIO TEST\n",
      "0.23333333333333334\n",
      "(145, 145) (145,)\n",
      "y_test: [1. 1. 1. 1. 1.] SVM: [1. 2. 1. 1. 1.]\n",
      "INICIO TEST\n",
      "0.26\n",
      "(145, 145) (145,)\n",
      "y_test: [1. 1. 1. 1. 1.] SVM: [1. 1. 3. 3. 1.]\n",
      "INICIO TEST\n",
      "0.28\n",
      "(145, 145) (145,)\n",
      "y_test: [2. 2. 2. 2. 2.] SVM: [2. 1. 2. 3. 1.]\n",
      "INICIO TEST\n",
      "0.29333333333333333\n",
      "(145, 145) (145,)\n",
      "y_test: [2. 2. 2. 2. 2.] SVM: [1. 1. 1. 3. 2.]\n",
      "INICIO TEST\n",
      "0.3\n",
      "(145, 145) (145,)\n",
      "y_test: [2. 2. 2. 2. 2.] SVM: [3. 3. 2. 1. 2.]\n",
      "INICIO TEST\n",
      "0.3133333333333334\n",
      "(145, 145) (145,)\n",
      "y_test: [2. 2. 2. 2. 2.] SVM: [2. 2. 2. 2. 2.]\n",
      "INICIO TEST\n",
      "0.34666666666666673\n",
      "(145, 145) (145,)\n",
      "y_test: [2. 2. 2. 2. 2.] SVM: [2. 2. 1. 1. 1.]\n",
      "INICIO TEST\n",
      "0.36000000000000004\n",
      "(145, 145) (145,)\n",
      "y_test: [2. 2. 2. 2. 2.] SVM: [2. 2. 2. 3. 2.]\n",
      "INICIO TEST\n",
      "0.3866666666666667\n",
      "(145, 145) (145,)\n",
      "y_test: [2. 2. 2. 2. 2.] SVM: [2. 2. 2. 2. 2.]\n",
      "INICIO TEST\n",
      "0.42000000000000004\n",
      "(145, 145) (145,)\n",
      "y_test: [2. 2. 2. 2. 2.] SVM: [3. 2. 3. 2. 1.]\n",
      "INICIO TEST\n",
      "0.43333333333333335\n",
      "(145, 145) (145,)\n",
      "y_test: [2. 2. 2. 2. 2.] SVM: [2. 2. 2. 2. 2.]\n",
      "INICIO TEST\n",
      "0.4666666666666667\n",
      "(145, 145) (145,)\n",
      "y_test: [2. 2. 2. 2. 2.] SVM: [2. 2. 2. 2. 3.]\n",
      "INICIO TEST\n",
      "0.49333333333333335\n",
      "(145, 145) (145,)\n",
      "y_test: [3. 3. 3. 3. 3.] SVM: [1. 3. 2. 1. 2.]\n",
      "INICIO TEST\n",
      "0.5\n",
      "(145, 145) (145,)\n",
      "y_test: [3. 3. 3. 3. 3.] SVM: [1. 2. 1. 2. 3.]\n",
      "INICIO TEST\n",
      "0.5066666666666666\n",
      "(145, 145) (145,)\n",
      "y_test: [3. 3. 3. 3. 3.] SVM: [1. 3. 1. 2. 3.]\n",
      "INICIO TEST\n",
      "0.52\n",
      "(145, 145) (145,)\n",
      "y_test: [3. 3. 3. 3. 3.] SVM: [1. 1. 2. 2. 2.]\n",
      "INICIO TEST\n",
      "0.52\n",
      "(145, 145) (145,)\n",
      "y_test: [3. 3. 3. 3. 3.] SVM: [2. 2. 2. 2. 1.]\n",
      "INICIO TEST\n",
      "0.52\n",
      "(145, 145) (145,)\n",
      "y_test: [3. 3. 3. 3. 3.] SVM: [1. 1. 1. 3. 1.]\n",
      "INICIO TEST\n",
      "0.5266666666666666\n",
      "(145, 145) (145,)\n",
      "y_test: [3. 3. 3. 3. 3.] SVM: [1. 3. 3. 1. 2.]\n",
      "INICIO TEST\n",
      "0.5399999999999999\n",
      "(145, 145) (145,)\n",
      "y_test: [3. 3. 3. 3. 3.] SVM: [1. 3. 1. 1. 1.]\n",
      "INICIO TEST\n",
      "0.5466666666666666\n",
      "(145, 145) (145,)\n",
      "y_test: [3. 3. 3. 3. 3.] SVM: [2. 3. 1. 2. 2.]\n",
      "INICIO TEST\n",
      "0.5533333333333332\n",
      "(145, 145) (145,)\n",
      "y_test: [3. 3. 3. 3. 3.] SVM: [1. 2. 2. 2. 2.]\n",
      "INICIO TEST\n",
      "0.5533333333333332\n",
      "()\n",
      "Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\n",
      "()\n",
      "Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\n",
      "(30, 40)\n",
      "Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\n",
      "()\n",
      "Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\n",
      "()\n",
      "Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\n",
      "SE GUARDÃ“ EL ARCHIVO sym8_valences_bandrois_easy_MKL\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.model_selection import KFold as KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#---Vars Definition-------------------------------------------------------------------------------------\n",
    "Valences=3\n",
    "CrossVal= KFold(n_splits=30,shuffle=False)\n",
    "N_Band= 40\n",
    "N_Trials= 150\n",
    "#C = 2.**np.arange(-15,16)\n",
    "#SVM_C=Moda(N_Emociones,Subjects,Best_C_Matrix)\n",
    "#N_cs = len(C)\n",
    "\n",
    "\n",
    "#---Iterations for each Emotion and subject--------------------------------------------------------------------------\n",
    "\n",
    "wave_name= ['bior3.3','bior6.8','sym2','sym5','sym8','coif1','coif5','db4','db10','db15']\n",
    "\n",
    "for wave in range(4,5):\n",
    "    File= open('Wavelets_Results_ROIs_2021/Features_Stats_Wavelets_'+wave_name[wave]+'_valences_bandrois'+'.pckl', 'rb')\n",
    "    Features_Stats_Wavelets = pickle.load(File)\n",
    "    File.close()\n",
    "    N_folds= CrossVal.get_n_splits(np.squeeze(Features_Stats_Wavelets[1,:,:]))\n",
    "    \n",
    "    F1_test_fold = np.zeros([N_folds])\n",
    "    ACC_test_fold = np.zeros([N_folds])\n",
    "    #SVM_Prediction_test=np.zeros([Valences,N_folds])\n",
    "    K = np.zeros([N_Band,N_Trials,N_Trials])\n",
    "    weights=np.zeros([N_folds,N_Band])\n",
    "\n",
    "    print(Features_Stats_Wavelets.shape)\n",
    "    \n",
    "    y_test = []\n",
    "    print('Analizando Wavelet: '+wave_name[wave]) \n",
    "\n",
    "    for band in range(0,N_Band):\n",
    "        scaler = MinMaxScaler()\n",
    "        K[band,:,:] = KerRBF(np.squeeze(Features_Stats_Wavelets[band,:,:]))\n",
    "        K[band,:,:] = scaler.fit_transform(K[band,:,:])\n",
    "        K[band,:,:] = normalize(K[band,:,:])\n",
    "\n",
    "    fold = 0\n",
    "\n",
    "    for train_index, test_index in CrossVal.split(np.squeeze(Features_Stats_Wavelets[1,:,:]),label_matrix):\n",
    "        ytrain, ytest = label_matrix[train_index], label_matrix[test_index]\n",
    "        ytrainMKL, ytestMKL = label_matrix[train_index], label_matrix[test_index]\n",
    "        ytrain2=np.expand_dims(ytrainMKL,axis=1)\n",
    "        y_test.append(ytest)\n",
    "\n",
    "        #Xtrain,Xtest = np.squeeze(Features_Stats_Wavelets[valence,band,train_index,:]), (Features_Stats_Wavelets[valence,band,test_index,:])\n",
    "\n",
    "    #---Definition of kernels variables for the multiple kernel learning--------------------------------------------------------\n",
    "\n",
    "        Ktrain = np.zeros([N_Band,len(train_index),len(train_index)])\n",
    "        Ktrain =  K[:,train_index,:]\n",
    "        Ktrain = Ktrain[:,:,train_index]\n",
    "        Ktest = np.zeros([N_Band,len(test_index),len(train_index)])\n",
    "        Ktest = K[:,test_index,:]\n",
    "        Ktest = Ktest[:,:,train_index]\n",
    "\n",
    "\n",
    "    #--- Multiple Kernel Learning GRAM---------------------------------------------------------------------\n",
    "\n",
    "        #earlystop = EarlyStopping(\n",
    "        #            Ktrain, ytrain,      #validation data, KL is a validation kernels list\n",
    "        #            patience=5,     #max number of acceptable negative steps\n",
    "        #            cooldown=1,     #how ofter we run a measurement, 1= every optimization step\n",
    "        #            metric='auc',   #the metric we monitor\n",
    "        #            )\n",
    "\n",
    "        #scheduler = ReduceOnWorsening()\n",
    "\n",
    "        #mkl = GRAM(\n",
    "        #max_iter=1000,          \n",
    "        #learning_rate=.01,      \n",
    "        #callbacks=[earlystop],\n",
    "        #scheduler=ReduceOnWorsening()).fit(Ktrain, ytrain)\n",
    "        #print(mkl)\n",
    "        #mkl.predict(Ktest)\n",
    "\n",
    "\n",
    "    #---Multiple Kernel Learning Custom Functions---------------------------------------------------------------------------\n",
    "\n",
    "#             clf_Linear_Easy = EasyMKL(lam=0.5,multiclass_strategy='ova').fit(Ktrain, ytrain)\t#a wrapper for averaging kernels\n",
    "        #clf_Linear_Easy = AverageMKL().fit(Ktrain,ytrain)\n",
    "#             weights_train = clf_Linear_Easy.weights\n",
    "        #print(weights_train)\n",
    "\n",
    "        weights_train = ckaweightedK(Ktrain,ytrain)\n",
    "\n",
    "        #weights_train = np.random.uniform(low=0.0, high=1.0, size=N_Band)\n",
    "        #weights_train = weights_train/np.sum(weights_train)\n",
    "\n",
    "        #aux=np.ones(len(weights_train))\n",
    "        #weights_train = aux-weights_train\n",
    "        #print(weights_train)\n",
    "        weights[fold]=weights_train\n",
    "        Kernel_train= MixKer(Ktrain,weights_train)\n",
    "        Kernel_test= MixKer(Ktest,weights_train)\n",
    "\n",
    "    #---Multiple Kernel Learning------------------------------------------------------------------------------------------------\n",
    "        \"\"\"\"\n",
    "        clf_Linear_Average = AverageMKL().fit(Ktrain,ytrain)\t#a wrapper for averaging kernels\n",
    "        #print ('done')\n",
    "        #print(clf_Linear_Average.weights)\t\t\t#print the weights of the combination of base kernels\n",
    "        K_Linear_average = clf_Linear_Average.ker_matrix\t#the combined kernel matrix\n",
    "        ##print(K_average)\n",
    "        #print ('training EasyMKL...', end='')\n",
    "        clf_Linear_Easy = EasyMKL().fit(Ktrain,ytrain)\t#a wrapper for averaging kernels\n",
    "        #print ('done')\n",
    "        #print(clf_Linear_Easy.weights)\t\t\t#print the weights of the combination of base kernels\n",
    "        K_Linear_easy = clf_Linear_Easy.ker_matrix\t#the combined kernel matrix\n",
    "        #print(K_Linear_average)\n",
    "        Ktrain = MixKer(Ktrain,clf_Linear_Easy.weights)\n",
    "        Ktest = MixKer(Ktest,clf_Linear_Easy.weights)\n",
    "        \"\"\"\n",
    "    #---F1-score evaluation for training set------------------------------------------------------------------------------------\n",
    "\n",
    "        #Distances= scipy.spatial.distance.pdist(Xtrain, metric='euclidean')\n",
    "        #Median= statistics.median(Distances)\n",
    "        Parameters_grid = svc_param_selection(ytrain,5,Kernel_train)\n",
    "        SVM_Classifier= svm.SVC(kernel='precomputed',C=Parameters_grid['C'],class_weight = 'balanced')\n",
    "        SVM_Classifier.fit(Kernel_train,ytrain)\n",
    "        SVM_Prediction_train= SVM_Classifier.predict(Kernel_train)\n",
    "        SVM_Prediction_test= SVM_Classifier.predict(Kernel_test)\n",
    "        F1_test_fold[fold] = f1_score(y_test[fold],SVM_Prediction_test,zero_division=1, average=\"micro\")\n",
    "#             ACC_test_fold[fold]= acc(y_test[fold],SVM_Prediction_test)\n",
    "        print(\"y_test:\",y_test[fold],\"SVM:\",SVM_Prediction_test)\n",
    "        #F1_train[sub,emo,fold,c]= f1_score(ytrain,SVM_Prediction_train,average='macro')\n",
    "\n",
    "\n",
    "        fold+=1\n",
    "\n",
    "        #---F1-score evaluation for test set---------------------------------------------------------------------------------------\n",
    "\n",
    "        print('INICIO TEST')\n",
    "        #Calcular F1_test\n",
    "        #print(y_test[sub,:],SVM_Prediction_test[sub,:,band,c])\n",
    "        F1_test = np.mean(F1_test_fold[:])\n",
    "        ACC_test= np.mean(ACC_test_fold[:])\n",
    "        STD_F1_test = np.std(F1_test_fold[:])\n",
    "        STD_ACC_test = np.std(ACC_test_fold[:])\n",
    "        print(F1_test)\n",
    "    #graficar(F1_test,Subjects,emocion)\n",
    "    \n",
    "    wav= wave_name[wave]+'_valences_bandrois_easy_MKL'\n",
    "    #Pickle Save F1 Score\n",
    "    path_save= \"Wavelets_Results_ROIs_2021/\"\n",
    "    \n",
    "    import pickle\n",
    "    file= open(path_save+'F1_Score_'+wav+'.pckl', 'wb')\n",
    "    pickle.dump(F1_test,file)\n",
    "    file.close()\n",
    "\n",
    "    File= open(path_save+'F1_Score_'+wav+'.pckl', 'rb')\n",
    "    Load = pickle.load(File)\n",
    "    File.close()\n",
    "    print(Load.shape)\n",
    "    if (Load == F1_test).all:\n",
    "        print(\"Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\")\n",
    "    else:\n",
    "        print(\"Sigue intentando\")\n",
    "\n",
    "    #Pickle Save Accuracy\n",
    "    file= open(path_save+'ACC_Score_'+wav+'.pckl', 'wb')\n",
    "    pickle.dump(ACC_test,file)\n",
    "    file.close()\n",
    "\n",
    "    File= open(path_save+'ACC_Score_'+wav+'.pckl', 'rb')\n",
    "    Load = pickle.load(File)\n",
    "    File.close()\n",
    "    print(Load.shape)\n",
    "    if (Load == ACC_test).all:\n",
    "        print(\"Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\")\n",
    "    else:\n",
    "        print(\"Sigue intentando\")\n",
    "    \n",
    "    #Pickle Save Weights\n",
    "    file= open(path_save+'Weights_'+wav+'.pckl', 'wb')\n",
    "    pickle.dump(weights,file)\n",
    "    file.close()\n",
    "\n",
    "    File= open(path_save+'Weights_'+wav+'.pckl', 'rb')\n",
    "    Load = pickle.load(File)\n",
    "    File.close()\n",
    "    print(Load.shape)\n",
    "    if (Load == weights).all:\n",
    "        print(\"Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\")\n",
    "    else:\n",
    "        print(\"Sigue intentando\")\n",
    "        \n",
    "    #Pickle Save STD\n",
    "    file= open(path_save+'STD_F1_'+wav+'.pckl', 'wb')\n",
    "    pickle.dump(STD_F1_test,file)\n",
    "    file.close()\n",
    "\n",
    "    File= open(path_save+'STD_F1_'+wav+'.pckl', 'rb')\n",
    "    Load = pickle.load(File)\n",
    "    File.close()\n",
    "    print(Load.shape)\n",
    "    if (Load == STD_F1_test).all:\n",
    "        print(\"Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\")\n",
    "    else:\n",
    "        print(\"Sigue intentando\")\n",
    "        \n",
    "        \n",
    "    #Pickle Save STD ACC\n",
    "    file= open(path_save+'STD_ACC_'+wav+'.pckl', 'wb')\n",
    "    pickle.dump(STD_ACC_test,file)\n",
    "    file.close()\n",
    "\n",
    "    File= open(path_save+'STD_ACC_'+wav+'.pckl', 'rb')\n",
    "    Load = pickle.load(File)\n",
    "    File.close()\n",
    "    print(Load.shape)\n",
    "    if (Load == STD_ACC_test).all:\n",
    "        print(\"Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\")\n",
    "    else:\n",
    "        print(\"Sigue intentando\")\n",
    "        \n",
    "    print('SE GUARDÃ“ EL ARCHIVO '+wav) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 3., 3.,\n",
       "       3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "       3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "       3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using 3 valences and 5 bands: \n",
    "### Here it was performed a classification for each valence and it was combined the ROIs and statistical features in a single dimension, keeping the bands dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 150, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Features_Stats_Wavelets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 150, 64)\n",
      "Analizando Wavelet: sym8\n",
      "(145, 145) (145,)\n",
      "y_test: [1. 1. 1. 1. 1.] SVM: [1. 1. 1. 1. 1.] ACC: 1.0 F1: 1.0 C: 1.0\n",
      "INICIO TEST\n",
      "0.03333333333333333\n",
      "(145, 145) (145,)\n",
      "y_test: [1. 1. 1. 1. 1.] SVM: [1. 1. 1. 1. 1.] ACC: 1.0 F1: 1.0 C: 1.0\n",
      "INICIO TEST\n",
      "0.06666666666666667\n",
      "(145, 145) (145,)\n",
      "y_test: [1. 1. 1. 1. 1.] SVM: [1. 1. 1. 1. 1.] ACC: 1.0 F1: 1.0 C: 1.0\n",
      "INICIO TEST\n",
      "0.1\n",
      "(145, 145) (145,)\n",
      "y_test: [1. 1. 1. 1. 1.] SVM: [1. 1. 1. 1. 1.] ACC: 1.0 F1: 1.0 C: 1.0\n",
      "INICIO TEST\n",
      "0.13333333333333333\n",
      "(145, 145) (145,)\n",
      "y_test: [1. 1. 1. 1. 1.] SVM: [1. 1. 1. 1. 1.] ACC: 1.0 F1: 1.0 C: 1.0\n",
      "INICIO TEST\n",
      "0.16666666666666666\n",
      "(145, 145) (145,)\n",
      "y_test: [1. 1. 1. 1. 1.] SVM: [1. 1. 1. 1. 1.] ACC: 1.0 F1: 1.0 C: 1.0\n",
      "INICIO TEST\n",
      "0.2\n",
      "(145, 145) (145,)\n",
      "y_test: [1. 1. 1. 1. 1.] SVM: [1. 1. 1. 1. 1.] ACC: 1.0 F1: 1.0 C: 1.0\n",
      "INICIO TEST\n",
      "0.23333333333333334\n",
      "(145, 145) (145,)\n",
      "y_test: [1. 1. 1. 1. 1.] SVM: [1. 1. 1. 1. 1.] ACC: 1.0 F1: 1.0 C: 1.0\n",
      "INICIO TEST\n",
      "0.26666666666666666\n",
      "(145, 145) (145,)\n",
      "y_test: [1. 1. 1. 1. 1.] SVM: [1. 1. 1. 1. 1.] ACC: 1.0 F1: 1.0 C: 1.0\n",
      "INICIO TEST\n",
      "0.3\n",
      "(145, 145) (145,)\n",
      "y_test: [1. 1. 1. 1. 1.] SVM: [1. 1. 1. 1. 1.] ACC: 1.0 F1: 1.0 C: 1.0\n",
      "INICIO TEST\n",
      "0.3333333333333333\n",
      "(145, 145) (145,)\n",
      "y_test: [2. 2. 2. 2. 2.] SVM: [2. 2. 2. 2. 2.] ACC: 1.0 F1: 1.0 C: 1.0\n",
      "INICIO TEST\n",
      "0.36666666666666664\n",
      "(145, 145) (145,)\n",
      "y_test: [2. 2. 2. 2. 2.] SVM: [2. 2. 2. 2. 2.] ACC: 1.0 F1: 1.0 C: 1.0\n",
      "INICIO TEST\n",
      "0.4\n",
      "(145, 145) (145,)\n",
      "y_test: [2. 2. 2. 2. 2.] SVM: [2. 2. 2. 2. 2.] ACC: 1.0 F1: 1.0 C: 1.0\n",
      "INICIO TEST\n",
      "0.43333333333333335\n",
      "(145, 145) (145,)\n",
      "y_test: [2. 2. 2. 2. 2.] SVM: [2. 2. 2. 2. 2.] ACC: 1.0 F1: 1.0 C: 1.0\n",
      "INICIO TEST\n",
      "0.4666666666666667\n",
      "(145, 145) (145,)\n",
      "y_test: [2. 2. 2. 2. 2.] SVM: [2. 2. 2. 2. 2.] ACC: 1.0 F1: 1.0 C: 1.0\n",
      "INICIO TEST\n",
      "0.5\n",
      "(145, 145) (145,)\n",
      "y_test: [2. 2. 2. 2. 2.] SVM: [2. 2. 2. 2. 2.] ACC: 1.0 F1: 1.0 C: 1.0\n",
      "INICIO TEST\n",
      "0.5333333333333333\n",
      "(145, 145) (145,)\n",
      "y_test: [2. 2. 2. 2. 2.] SVM: [2. 2. 2. 2. 2.] ACC: 1.0 F1: 1.0 C: 1.0\n",
      "INICIO TEST\n",
      "0.5666666666666667\n",
      "(145, 145) (145,)\n",
      "y_test: [2. 2. 2. 2. 2.] SVM: [2. 2. 2. 2. 2.] ACC: 1.0 F1: 1.0 C: 1.0\n",
      "INICIO TEST\n",
      "0.6\n",
      "(145, 145) (145,)\n",
      "y_test: [2. 2. 2. 2. 2.] SVM: [2. 2. 2. 2. 2.] ACC: 1.0 F1: 1.0 C: 1.0\n",
      "INICIO TEST\n",
      "0.6333333333333333\n",
      "(145, 145) (145,)\n",
      "y_test: [2. 2. 2. 2. 2.] SVM: [2. 2. 2. 2. 2.] ACC: 1.0 F1: 1.0 C: 1.0\n",
      "INICIO TEST\n",
      "0.6666666666666666\n",
      "(145, 145) (145,)\n",
      "y_test: [3. 3. 3. 3. 3.] SVM: [3. 3. 3. 3. 3.] ACC: 1.0 F1: 1.0 C: 1.0\n",
      "INICIO TEST\n",
      "0.7\n",
      "(145, 145) (145,)\n",
      "y_test: [3. 3. 3. 3. 3.] SVM: [2. 1. 1. 2. 3.] ACC: 0.2 F1: 0.20000000000000004 C: 1.0\n",
      "INICIO TEST\n",
      "0.7066666666666667\n",
      "(145, 145) (145,)\n",
      "y_test: [3. 3. 3. 3. 3.] SVM: [2. 3. 3. 3. 3.] ACC: 0.8 F1: 0.8000000000000002 C: 1.0\n",
      "INICIO TEST\n",
      "0.7333333333333333\n",
      "(145, 145) (145,)\n",
      "y_test: [3. 3. 3. 3. 3.] SVM: [3. 3. 3. 3. 1.] ACC: 0.8 F1: 0.8000000000000002 C: 1.0\n",
      "INICIO TEST\n",
      "0.76\n",
      "(145, 145) (145,)\n",
      "y_test: [3. 3. 3. 3. 3.] SVM: [3. 3. 3. 3. 3.] ACC: 1.0 F1: 1.0 C: 1.0\n",
      "INICIO TEST\n",
      "0.7933333333333333\n",
      "(145, 145) (145,)\n",
      "y_test: [3. 3. 3. 3. 3.] SVM: [3. 3. 3. 2. 3.] ACC: 0.8 F1: 0.8000000000000002 C: 1.0\n",
      "INICIO TEST\n",
      "0.8200000000000001\n",
      "(145, 145) (145,)\n",
      "y_test: [3. 3. 3. 3. 3.] SVM: [3. 3. 2. 2. 3.] ACC: 0.6 F1: 0.6 C: 1.0\n",
      "INICIO TEST\n",
      "0.8400000000000001\n",
      "(145, 145) (145,)\n",
      "y_test: [3. 3. 3. 3. 3.] SVM: [3. 3. 3. 3. 2.] ACC: 0.8 F1: 0.8000000000000002 C: 1.0\n",
      "INICIO TEST\n",
      "0.8666666666666668\n",
      "(145, 145) (145,)\n",
      "y_test: [3. 3. 3. 3. 3.] SVM: [3. 3. 3. 3. 3.] ACC: 1.0 F1: 1.0 C: 1.0\n",
      "INICIO TEST\n",
      "0.9000000000000001\n",
      "(145, 145) (145,)\n",
      "y_test: [3. 3. 3. 3. 3.] SVM: [3. 3. 3. 1. 3.] ACC: 0.8 F1: 0.8000000000000002 C: 1.0\n",
      "INICIO TEST\n",
      "0.9266666666666669\n",
      "()\n",
      "Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\n",
      "()\n",
      "Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\n",
      "(30, 5)\n",
      "Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\n",
      "()\n",
      "Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\n",
      "()\n",
      "Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\n",
      "SE GUARDÃ“ EL ARCHIVO sym8_valences_bands_easy_MKL\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.model_selection import KFold as KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#---Vars Definition-------------------------------------------------------------------------------------\n",
    "CrossVal= KFold(n_splits=30,shuffle=False)\n",
    "N_Band= 5\n",
    "N_Trials= 150\n",
    "#C = 2.**np.arange(-15,16)\n",
    "#SVM_C=Moda(N_Emociones,Subjects,Best_C_Matrix)\n",
    "#N_cs = len(C)\n",
    "\n",
    "\n",
    "#---Iterations for each Emotion and subject--------------------------------------------------------------------------\n",
    "\n",
    "wave_name= ['bior3.3','bior6.8','sym2','sym5','sym8','coif1','coif5','db4','db10','db15']\n",
    "\n",
    "for wave in range(4,5):\n",
    "    File= open('Wavelets_Results_ROIs_2021/Features_Stats_Wavelets_'+wave_name[wave]+'_valences_bands'+'.pckl', 'rb')\n",
    "    Features_Stats_Wavelets = pickle.load(File)\n",
    "    File.close()\n",
    "    N_folds= CrossVal.get_n_splits(np.squeeze(Features_Stats_Wavelets[1,:,:]))\n",
    "    \n",
    "    F1_test_fold = np.zeros([N_folds])\n",
    "    ACC_test_fold = np.zeros([N_folds])\n",
    "    #SVM_Prediction_test=np.zeros([Valences,N_folds])\n",
    "    K = np.zeros([N_Band,N_Trials,N_Trials])\n",
    "    weights=np.zeros([N_folds,N_Band])\n",
    "\n",
    "    print(Features_Stats_Wavelets.shape)\n",
    "    \n",
    "    y_test = []\n",
    "    print('Analizando Wavelet: '+wave_name[wave]) \n",
    "\n",
    "    for band in range(0,N_Band):\n",
    "        scaler = MinMaxScaler()\n",
    "        K[band,:,:] = KerRBF(np.squeeze(Features_Stats_Wavelets[band,:,:]))\n",
    "        K[band,:,:] = scaler.fit_transform(K[band,:,:])\n",
    "        K[band,:,:] = normalize(K[band,:,:])\n",
    "\n",
    "    fold = 0\n",
    "\n",
    "    for train_index, test_index in CrossVal.split(np.squeeze(Features_Stats_Wavelets[1,:,:])):\n",
    "        ytrain, ytest = label_matrix[train_index], label_matrix[test_index]\n",
    "        ytrainMKL, ytestMKL = label_matrix[train_index], label_matrix[test_index]\n",
    "        ytrain2=np.expand_dims(ytrainMKL,axis=1)\n",
    "        y_test.append(ytest)\n",
    "\n",
    "        #Xtrain,Xtest = np.squeeze(Features_Stats_Wavelets[valence,band,train_index,:]), (Features_Stats_Wavelets[valence,band,test_index,:])\n",
    "\n",
    "    #---Definition of kernels variables for the multiple kernel learning--------------------------------------------------------\n",
    "\n",
    "        Ktrain = np.zeros([N_Band,len(train_index),len(train_index)])\n",
    "        Ktrain =  K[:,train_index,:]\n",
    "        Ktrain = Ktrain[:,:,train_index]\n",
    "        Ktest = np.zeros([N_Band,len(test_index),len(train_index)])\n",
    "        Ktest = K[:,test_index,:]\n",
    "        Ktest = Ktest[:,:,train_index]\n",
    "\n",
    "\n",
    "    #--- Multiple Kernel Learning GRAM---------------------------------------------------------------------\n",
    "\n",
    "        #earlystop = EarlyStopping(\n",
    "        #            Ktrain, ytrain,      #validation data, KL is a validation kernels list\n",
    "        #            patience=5,     #max number of acceptable negative steps\n",
    "        #            cooldown=1,     #how ofter we run a measurement, 1= every optimization step\n",
    "        #            metric='auc',   #the metric we monitor\n",
    "        #            )\n",
    "\n",
    "        #scheduler = ReduceOnWorsening()\n",
    "\n",
    "        #mkl = GRAM(\n",
    "        #max_iter=1000,          \n",
    "        #learning_rate=.01,      \n",
    "        #callbacks=[earlystop],\n",
    "        #scheduler=ReduceOnWorsening()).fit(Ktrain, ytrain)\n",
    "        #print(mkl)\n",
    "        #mkl.predict(Ktest)\n",
    "\n",
    "\n",
    "    #---Multiple Kernel Learning Custom Functions---------------------------------------------------------------------------\n",
    "\n",
    "#         clf_Linear_Easy = EasyMKL(lam=1).fit(Ktrain,ytrain)\t#a wrapper for averaging kernels\n",
    "        #clf_Linear_Easy = AverageMKL().fit(Ktrain,ytrain)\n",
    "#         weights_train = clf_Linear_Easy.weights\n",
    "        #print(weights_train)\n",
    "\n",
    "        weights_train = ckaweightedK(Ktrain,ytrain)\n",
    "\n",
    "        #weights_train = np.random.uniform(low=0.0, high=1.0, size=N_Band)\n",
    "        #weights_train = weights_train/np.sum(weights_train)\n",
    "\n",
    "        #aux=np.ones(len(weights_train))\n",
    "        #weights_train = aux-weights_train\n",
    "        #print(weights_train)\n",
    "        weights[fold]=weights_train\n",
    "        Kernel_train= MixKer(Ktrain,weights_train)\n",
    "        Kernel_test= MixKer(Ktest,weights_train)\n",
    "\n",
    "    #---Multiple Kernel Learning------------------------------------------------------------------------------------------------\n",
    "        \"\"\"\"\n",
    "        clf_Linear_Average = AverageMKL().fit(Ktrain,ytrain)\t#a wrapper for averaging kernels\n",
    "        #print ('done')\n",
    "        #print(clf_Linear_Average.weights)\t\t\t#print the weights of the combination of base kernels\n",
    "        K_Linear_average = clf_Linear_Average.ker_matrix\t#the combined kernel matrix\n",
    "        ##print(K_average)\n",
    "        #print ('training EasyMKL...', end='')\n",
    "        clf_Linear_Easy = EasyMKL().fit(Ktrain,ytrain)\t#a wrapper for averaging kernels\n",
    "        #print ('done')\n",
    "        #print(clf_Linear_Easy.weights)\t\t\t#print the weights of the combination of base kernels\n",
    "        K_Linear_easy = clf_Linear_Easy.ker_matrix\t#the combined kernel matrix\n",
    "        #print(K_Linear_average)\n",
    "        Ktrain = MixKer(Ktrain,clf_Linear_Easy.weights)\n",
    "        Ktest = MixKer(Ktest,clf_Linear_Easy.weights)\n",
    "        \"\"\"\n",
    "    #---F1-score evaluation for training set------------------------------------------------------------------------------------\n",
    "\n",
    "        #Distances= scipy.spatial.distance.pdist(Xtrain, metric='euclidean')\n",
    "        #Median= statistics.median(Distances)\n",
    "        Parameters_grid = svc_param_selection(ytrain,5,Kernel_train)\n",
    "        SVM_Classifier= svm.SVC(kernel='precomputed',C=Parameters_grid['C'],class_weight = 'balanced')\n",
    "        SVM_Classifier.fit(Kernel_train,ytrain)\n",
    "        SVM_Prediction_train= SVM_Classifier.predict(Kernel_train)\n",
    "        SVM_Prediction_test= SVM_Classifier.predict(Kernel_test)\n",
    "        F1_test_fold[fold] = f1_score(y_test[fold],SVM_Prediction_test,zero_division=1, average=\"micro\")\n",
    "        ACC_test_fold[fold]= acc(y_test[fold],SVM_Prediction_test)\n",
    "        print(\"y_test:\",y_test[fold],\"SVM:\",SVM_Prediction_test,\"ACC:\",ACC_test_fold[fold],\"F1:\",F1_test_fold[fold],\"C:\",Parameters_grid['C'])\n",
    "        #F1_train[sub,emo,fold,c]= f1_score(ytrain,SVM_Prediction_train,average='macro')\n",
    "\n",
    "\n",
    "        fold+=1\n",
    "\n",
    "        #---F1-score evaluation for test set---------------------------------------------------------------------------------------\n",
    "\n",
    "        print('INICIO TEST')\n",
    "        #Calcular F1_test\n",
    "        #print(y_test[sub,:],SVM_Prediction_test[sub,:,band,c])\n",
    "        F1_test = np.mean(F1_test_fold[:])\n",
    "        ACC_test= np.mean(ACC_test_fold[:])\n",
    "        STD_F1_test = np.std(F1_test_fold[:])\n",
    "        STD_ACC_test = np.std(ACC_test_fold[:])\n",
    "        print(ACC_test)\n",
    "    #graficar(F1_test,Subjects,emocion)\n",
    "    \n",
    "    wav= wave_name[wave]+'_valences_bands_easy_MKL'\n",
    "    #Pickle Save F1 Score\n",
    "    path_save= \"Wavelets_Results_ROIs_2021/\"\n",
    "    \n",
    "    import pickle\n",
    "    file= open(path_save+'F1_Score_'+wav+'.pckl', 'wb')\n",
    "    pickle.dump(F1_test,file)\n",
    "    file.close()\n",
    "\n",
    "    File= open(path_save+'F1_Score_'+wav+'.pckl', 'rb')\n",
    "    Load = pickle.load(File)\n",
    "    File.close()\n",
    "    print(Load.shape)\n",
    "    if (Load == F1_test).all:\n",
    "        print(\"Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\")\n",
    "    else:\n",
    "        print(\"Sigue intentando\")\n",
    "\n",
    "    #Pickle Save Accuracy\n",
    "    file= open(path_save+'ACC_Score_'+wav+'.pckl', 'wb')\n",
    "    pickle.dump(ACC_test,file)\n",
    "    file.close()\n",
    "\n",
    "    File= open(path_save+'ACC_Score_'+wav+'.pckl', 'rb')\n",
    "    Load = pickle.load(File)\n",
    "    File.close()\n",
    "    print(Load.shape)\n",
    "    if (Load == ACC_test).all:\n",
    "        print(\"Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\")\n",
    "    else:\n",
    "        print(\"Sigue intentando\")\n",
    "    \n",
    "    #Pickle Save Weights\n",
    "    file= open(path_save+'Weights_'+wav+'.pckl', 'wb')\n",
    "    pickle.dump(weights,file)\n",
    "    file.close()\n",
    "\n",
    "    File= open(path_save+'Weights_'+wav+'.pckl', 'rb')\n",
    "    Load = pickle.load(File)\n",
    "    File.close()\n",
    "    print(Load.shape)\n",
    "    if (Load == weights).all:\n",
    "        print(\"Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\")\n",
    "    else:\n",
    "        print(\"Sigue intentando\")\n",
    "        \n",
    "    #Pickle Save STD\n",
    "    file= open(path_save+'STD_F1_'+wav+'.pckl', 'wb')\n",
    "    pickle.dump(STD_F1_test,file)\n",
    "    file.close()\n",
    "\n",
    "    File= open(path_save+'STD_F1_'+wav+'.pckl', 'rb')\n",
    "    Load = pickle.load(File)\n",
    "    File.close()\n",
    "    print(Load.shape)\n",
    "    if (Load == STD_F1_test).all:\n",
    "        print(\"Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\")\n",
    "    else:\n",
    "        print(\"Sigue intentando\")\n",
    "        \n",
    "        \n",
    "    #Pickle Save STD ACC\n",
    "    file= open(path_save+'STD_ACC_'+wav+'.pckl', 'wb')\n",
    "    pickle.dump(STD_ACC_test,file)\n",
    "    file.close()\n",
    "\n",
    "    File= open(path_save+'STD_ACC_'+wav+'.pckl', 'rb')\n",
    "    Load = pickle.load(File)\n",
    "    File.close()\n",
    "    print(Load.shape)\n",
    "    if (Load == STD_ACC_test).all:\n",
    "        print(\"Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\")\n",
    "    else:\n",
    "        print(\"Sigue intentando\")\n",
    "        \n",
    "    print('SE GUARDÃ“ EL ARCHIVO '+wav) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.23738254e-01, -1.40438497e-01,  5.77300101e-01,\n",
       "         1.37126302e-01, -7.60354650e-01],\n",
       "       [ 3.29379917e-01, -1.33213689e-01,  4.93614762e-01,\n",
       "         1.14672783e-01, -7.85466488e-01],\n",
       "       [ 3.27060636e-01, -1.10361482e-01,  4.88668455e-01,\n",
       "         1.00627962e-01, -7.94939518e-01],\n",
       "       [ 4.94717698e-02,  1.47528498e-01,  5.56571347e-01,\n",
       "         8.80083959e-02, -8.11338859e-01],\n",
       "       [ 3.08400300e-01, -8.13975630e-02,  4.88339787e-01,\n",
       "         1.08892753e-01, -8.04941186e-01],\n",
       "       [ 1.82398395e-01, -4.71972440e-02,  5.61574844e-01,\n",
       "         1.31016717e-01, -7.94966389e-01],\n",
       "       [ 1.74666724e-01,  4.83980205e-02,  5.21828585e-01,\n",
       "         1.04610498e-01, -8.26982913e-01],\n",
       "       [ 1.56213002e-01, -7.92205280e-02,  5.54299760e-01,\n",
       "         1.84395198e-01, -7.92509806e-01],\n",
       "       [ 2.53144855e-01, -1.64209083e-01,  5.60220973e-01,\n",
       "         1.36622859e-01, -7.59236271e-01],\n",
       "       [ 1.60669781e-01, -1.15425629e-01,  4.94875127e-01,\n",
       "         2.88222491e-01, -7.95542928e-01],\n",
       "       [ 2.64872466e-01, -6.62102672e-02,  5.07407997e-01,\n",
       "         1.24271283e-01, -8.07807248e-01],\n",
       "       [ 2.72712545e-01, -9.34885976e-02,  5.16687990e-01,\n",
       "         1.21896263e-01, -7.96908133e-01],\n",
       "       [ 2.75971987e-01, -6.50699648e-02,  5.05371844e-01,\n",
       "         1.12299324e-01, -8.07213431e-01],\n",
       "       [ 2.55489380e-01, -7.46497417e-02,  5.07293980e-01,\n",
       "         1.42175882e-01, -8.07212134e-01],\n",
       "       [ 2.46880290e-01, -8.14502617e-02,  5.30146018e-01,\n",
       "         1.24109337e-01, -7.97469780e-01],\n",
       "       [ 2.68008439e-01, -9.23637735e-02,  5.22461210e-01,\n",
       "         1.19027304e-01, -7.95303210e-01],\n",
       "       [ 2.91849403e-01, -3.01373908e-02,  4.82678421e-01,\n",
       "         9.89821953e-02, -8.19231183e-01],\n",
       "       [ 3.09162251e-01, -9.97930965e-02,  4.97753653e-01,\n",
       "         1.09562296e-01, -7.96679010e-01],\n",
       "       [ 3.14213802e-01, -1.29503544e-01,  5.00208861e-01,\n",
       "         1.22270432e-01, -7.86981293e-01],\n",
       "       [ 2.41271854e-01, -5.71785621e-02,  5.25317060e-01,\n",
       "         1.19597411e-01, -8.05144056e-01],\n",
       "       [ 3.66425139e-01, -1.39260107e-01,  4.87254522e-01,\n",
       "         8.06532302e-02, -7.76155478e-01],\n",
       "       [ 1.48954040e-01, -1.38274546e-01,  5.84098126e-01,\n",
       "         1.99273428e-01, -7.60139674e-01],\n",
       "       [ 1.29797897e-01, -1.87113533e-04,  5.48102333e-01,\n",
       "         1.63709394e-01, -8.09898474e-01],\n",
       "       [ 3.21029265e-01, -1.11312102e-01,  4.86299545e-01,\n",
       "         1.18432345e-01, -7.96264001e-01],\n",
       "       [ 2.14537476e-01, -6.74989205e-02,  5.11392411e-01,\n",
       "         1.80675398e-01, -8.09476232e-01],\n",
       "       [ 2.38573713e-01, -1.12932050e-01,  5.27878531e-01,\n",
       "         1.66432850e-01, -7.89919805e-01],\n",
       "       [ 2.84097446e-01, -1.18344142e-01,  5.90706568e-01,\n",
       "         5.04443358e-03, -7.45871041e-01],\n",
       "       [ 2.25103368e-01,  1.10378967e-02,  4.79089635e-01,\n",
       "         1.47999172e-01, -8.35329878e-01],\n",
       "       [-2.51899848e-02, -6.74933177e-02,  7.03690504e-01,\n",
       "         8.16365443e-02, -7.02114853e-01],\n",
       "       [ 3.80753718e-01, -6.18901521e-02,  5.77520429e-01,\n",
       "        -1.97600090e-01, -6.91824092e-01]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using 3 valences and 8 Rois: \n",
    "### Here it was performed a classification for each valence and it was combined bands and statistical features in a single dimension, keeping the rois dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 150, 40)\n",
      "Analizando Wavelet: sym8\n",
      "(145, 145) (145,)\n",
      "y_test: [1. 1. 1. 1. 1.] SVM: [1. 1. 1. 1. 1.] F1: 1.0 C: 1.0\n",
      "(145, 145) (145,)\n",
      "y_test: [1. 1. 1. 1. 1.] SVM: [1. 1. 1. 1. 1.] F1: 1.0 C: 1.0\n",
      "(145, 145) (145,)\n",
      "y_test: [1. 1. 1. 1. 1.] SVM: [1. 1. 1. 1. 1.] F1: 1.0 C: 1.0\n",
      "(145, 145) (145,)\n",
      "y_test: [1. 1. 1. 1. 1.] SVM: [1. 1. 1. 1. 1.] F1: 1.0 C: 1.0\n",
      "(145, 145) (145,)\n",
      "y_test: [1. 1. 1. 1. 1.] SVM: [1. 1. 1. 1. 1.] F1: 1.0 C: 1.0\n",
      "(145, 145) (145,)\n",
      "y_test: [1. 1. 1. 1. 1.] SVM: [1. 1. 1. 1. 1.] F1: 1.0 C: 1.0\n",
      "(145, 145) (145,)\n",
      "y_test: [1. 1. 1. 1. 1.] SVM: [1. 1. 1. 1. 1.] F1: 1.0 C: 1.0\n",
      "(145, 145) (145,)\n",
      "y_test: [1. 1. 1. 1. 1.] SVM: [1. 1. 1. 1. 1.] F1: 1.0 C: 1.0\n",
      "(145, 145) (145,)\n",
      "y_test: [1. 1. 1. 1. 1.] SVM: [1. 1. 1. 1. 1.] F1: 1.0 C: 1.0\n",
      "(145, 145) (145,)\n",
      "y_test: [1. 1. 1. 1. 1.] SVM: [1. 1. 1. 1. 1.] F1: 1.0 C: 1.0\n",
      "(145, 145) (145,)\n",
      "y_test: [2. 2. 2. 2. 2.] SVM: [2. 2. 2. 2. 2.] F1: 1.0 C: 1.0\n",
      "(145, 145) (145,)\n",
      "y_test: [2. 2. 2. 2. 2.] SVM: [2. 2. 2. 2. 2.] F1: 1.0 C: 1.0\n",
      "(145, 145) (145,)\n",
      "y_test: [2. 2. 2. 2. 2.] SVM: [2. 2. 2. 2. 2.] F1: 1.0 C: 1.0\n",
      "(145, 145) (145,)\n",
      "y_test: [2. 2. 2. 2. 2.] SVM: [2. 2. 2. 2. 2.] F1: 1.0 C: 1.0\n",
      "(145, 145) (145,)\n",
      "y_test: [2. 2. 2. 2. 2.] SVM: [2. 2. 2. 2. 2.] F1: 1.0 C: 1.0\n",
      "(145, 145) (145,)\n",
      "y_test: [2. 2. 2. 2. 2.] SVM: [2. 2. 2. 2. 2.] F1: 1.0 C: 1.0\n",
      "(145, 145) (145,)\n",
      "y_test: [2. 2. 2. 2. 2.] SVM: [2. 2. 2. 2. 2.] F1: 1.0 C: 1.0\n",
      "(145, 145) (145,)\n",
      "y_test: [2. 2. 2. 2. 2.] SVM: [3. 2. 2. 2. 2.] F1: 0.8000000000000002 C: 1.0\n",
      "(145, 145) (145,)\n",
      "y_test: [2. 2. 2. 2. 2.] SVM: [2. 2. 2. 2. 2.] F1: 1.0 C: 1.0\n",
      "(145, 145) (145,)\n",
      "y_test: [2. 2. 2. 2. 2.] SVM: [2. 2. 2. 2. 2.] F1: 1.0 C: 1.0\n",
      "(145, 145) (145,)\n",
      "y_test: [3. 3. 3. 3. 3.] SVM: [1. 2. 2. 2. 1.] F1: 0.0 C: 1.0\n",
      "(145, 145) (145,)\n",
      "y_test: [3. 3. 3. 3. 3.] SVM: [3. 3. 3. 1. 3.] F1: 0.8000000000000002 C: 1.0\n",
      "(145, 145) (145,)\n",
      "y_test: [3. 3. 3. 3. 3.] SVM: [2. 2. 1. 2. 1.] F1: 0.0 C: 1.0\n",
      "(145, 145) (145,)\n",
      "y_test: [3. 3. 3. 3. 3.] SVM: [3. 3. 3. 3. 2.] F1: 0.8000000000000002 C: 1.0\n",
      "(145, 145) (145,)\n",
      "y_test: [3. 3. 3. 3. 3.] SVM: [2. 1. 3. 2. 2.] F1: 0.20000000000000004 C: 1.0\n",
      "(145, 145) (145,)\n",
      "y_test: [3. 3. 3. 3. 3.] SVM: [3. 2. 2. 3. 3.] F1: 0.6 C: 1.0\n",
      "(145, 145) (145,)\n",
      "y_test: [3. 3. 3. 3. 3.] SVM: [3. 3. 3. 2. 2.] F1: 0.6 C: 1.0\n",
      "(145, 145) (145,)\n",
      "y_test: [3. 3. 3. 3. 3.] SVM: [3. 3. 1. 3. 3.] F1: 0.8000000000000002 C: 1.0\n",
      "(145, 145) (145,)\n",
      "y_test: [3. 3. 3. 3. 3.] SVM: [2. 2. 2. 2. 2.] F1: 0.0 C: 1.0\n",
      "(145, 145) (145,)\n",
      "y_test: [3. 3. 3. 3. 3.] SVM: [2. 2. 2. 2. 2.] F1: 0.0 C: 1.0\n",
      "INICIO TEST\n",
      "0.7866666666666667\n",
      "()\n",
      "Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\n",
      "()\n",
      "Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\n",
      "(30, 8)\n",
      "Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\n",
      "()\n",
      "Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\n",
      "()\n",
      "Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\n",
      "SE GUARDÃ“ EL ARCHIVO sym8_valences_rois_easy_MKL\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.model_selection import KFold as KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#---Vars Definition-------------------------------------------------------------------------------------\n",
    "Valences=3\n",
    "CrossVal= KFold(n_splits=30,shuffle=False)\n",
    "N_Band= 8\n",
    "N_Trials= 150\n",
    "#C = 2.**np.arange(-15,16)\n",
    "#SVM_C=Moda(N_Emociones,Subjects,Best_C_Matrix)\n",
    "#N_cs = len(C)\n",
    "\n",
    "\n",
    "#---Iterations for each Emotion and subject--------------------------------------------------------------------------\n",
    "\n",
    "wave_name= ['bior3.3','bior6.8','sym2','sym5','sym8','coif1','coif5','db4','db10','db15']\n",
    "\n",
    "for wave in range(4,5):\n",
    "    File= open('Wavelets_Results_ROIs_2021/Features_Stats_Wavelets_'+wave_name[wave]+'_valences_rois'+'.pckl', 'rb')\n",
    "    Features_Stats_Wavelets = pickle.load(File)\n",
    "    File.close()\n",
    "    N_folds= CrossVal.get_n_splits(np.squeeze(Features_Stats_Wavelets[1,:,:]))\n",
    "    \n",
    "    F1_test_fold = np.zeros([N_folds])\n",
    "    ACC_test_fold = np.zeros([N_folds])\n",
    "    #SVM_Prediction_test=np.zeros([Valences,N_folds])\n",
    "    K = np.zeros([N_Band,N_Trials,N_Trials])\n",
    "    weights=np.zeros([N_folds,N_Band])\n",
    "\n",
    "    print(Features_Stats_Wavelets.shape)\n",
    "    \n",
    "    y_test = []\n",
    "    print('Analizando Wavelet: '+wave_name[wave]) \n",
    "\n",
    "    for band in range(0,N_Band):\n",
    "        scaler = MinMaxScaler()\n",
    "        K[band,:,:] = KerRBF(np.squeeze(Features_Stats_Wavelets[band,:,:]))\n",
    "        K[band,:,:] = scaler.fit_transform(K[band,:,:])\n",
    "        K[band,:,:] = normalize(K[band,:,:])\n",
    "\n",
    "    fold = 0\n",
    "\n",
    "    for train_index, test_index in CrossVal.split(np.squeeze(Features_Stats_Wavelets[1,:,:])):\n",
    "        ytrain, ytest = label_matrix[train_index], label_matrix[test_index]\n",
    "        ytrainMKL, ytestMKL = label_matrix[train_index], label_matrix[test_index]\n",
    "        ytrain2=np.expand_dims(ytrainMKL,axis=1)\n",
    "        y_test.append(ytest)\n",
    "\n",
    "        #Xtrain,Xtest = np.squeeze(Features_Stats_Wavelets[valence,band,train_index,:]), (Features_Stats_Wavelets[valence,band,test_index,:])\n",
    "\n",
    "    #---Definition of kernels variables for the multiple kernel learning--------------------------------------------------------\n",
    "\n",
    "        Ktrain = np.zeros([N_Band,len(train_index),len(train_index)])\n",
    "        Ktrain =  K[:,train_index,:]\n",
    "        Ktrain = Ktrain[:,:,train_index]\n",
    "        Ktest = np.zeros([N_Band,len(test_index),len(train_index)])\n",
    "        Ktest = K[:,test_index,:]\n",
    "        Ktest = Ktest[:,:,train_index]\n",
    "\n",
    "\n",
    "    #--- Multiple Kernel Learning GRAM---------------------------------------------------------------------\n",
    "\n",
    "        #earlystop = EarlyStopping(\n",
    "        #            Ktrain, ytrain,      #validation data, KL is a validation kernels list\n",
    "        #            patience=5,     #max number of acceptable negative steps\n",
    "        #            cooldown=1,     #how ofter we run a measurement, 1= every optimization step\n",
    "        #            metric='auc',   #the metric we monitor\n",
    "        #            )\n",
    "\n",
    "        #scheduler = ReduceOnWorsening()\n",
    "\n",
    "        #mkl = GRAM(\n",
    "        #max_iter=1000,          \n",
    "        #learning_rate=.01,      \n",
    "        #callbacks=[earlystop],\n",
    "        #scheduler=ReduceOnWorsening()).fit(Ktrain, ytrain)\n",
    "        #print(mkl)\n",
    "        #mkl.predict(Ktest)\n",
    "\n",
    "\n",
    "    #---Multiple Kernel Learning Custom Functions---------------------------------------------------------------------------\n",
    "\n",
    "#         clf_Linear_Easy = EasyMKL(lam=1).fit(Ktrain,ytrain)\t#a wrapper for averaging kernels\n",
    "        #clf_Linear_Easy = AverageMKL().fit(Ktrain,ytrain)\n",
    "#         weights_train = clf_Linear_Easy.weights\n",
    "        #print(weights_train)\n",
    "\n",
    "        weights_train = ckaweightedK(Ktrain,ytrain)\n",
    "\n",
    "        #weights_train = np.random.uniform(low=0.0, high=1.0, size=N_Band)\n",
    "        #weights_train = weights_train/np.sum(weights_train)\n",
    "\n",
    "        #aux=np.ones(len(weights_train))\n",
    "        #weights_train = aux-weights_train\n",
    "        #print(weights_train)\n",
    "        weights[fold]=weights_train\n",
    "        Kernel_train= MixKer(Ktrain,weights_train)\n",
    "        Kernel_test= MixKer(Ktest,weights_train)\n",
    "\n",
    "    #---Multiple Kernel Learning------------------------------------------------------------------------------------------------\n",
    "        \"\"\"\"\n",
    "        clf_Linear_Average = AverageMKL().fit(Ktrain,ytrain)\t#a wrapper for averaging kernels\n",
    "        #print ('done')\n",
    "        #print(clf_Linear_Average.weights)\t\t\t#print the weights of the combination of base kernels\n",
    "        K_Linear_average = clf_Linear_Average.ker_matrix\t#the combined kernel matrix\n",
    "        ##print(K_average)\n",
    "        #print ('training EasyMKL...', end='')\n",
    "        clf_Linear_Easy = EasyMKL().fit(Ktrain,ytrain)\t#a wrapper for averaging kernels\n",
    "        #print ('done')\n",
    "        #print(clf_Linear_Easy.weights)\t\t\t#print the weights of the combination of base kernels\n",
    "        K_Linear_easy = clf_Linear_Easy.ker_matrix\t#the combined kernel matrix\n",
    "        #print(K_Linear_average)\n",
    "        Ktrain = MixKer(Ktrain,clf_Linear_Easy.weights)\n",
    "        Ktest = MixKer(Ktest,clf_Linear_Easy.weights)\n",
    "        \"\"\"\n",
    "    #---F1-score evaluation for training set------------------------------------------------------------------------------------\n",
    "\n",
    "        #Distances= scipy.spatial.distance.pdist(Xtrain, metric='euclidean')\n",
    "        #Median= statistics.median(Distances)\n",
    "        Parameters_grid = svc_param_selection(ytrain,5,Kernel_train)\n",
    "        SVM_Classifier= svm.SVC(kernel='precomputed',C=Parameters_grid['C'],class_weight = 'balanced')\n",
    "        SVM_Classifier.fit(Kernel_train,ytrain)\n",
    "        SVM_Prediction_train= SVM_Classifier.predict(Kernel_train)\n",
    "        SVM_Prediction_test= SVM_Classifier.predict(Kernel_test)\n",
    "        F1_test_fold[fold] = f1_score(y_test[fold],SVM_Prediction_test,zero_division=1, average=\"micro\")\n",
    "        ACC_test_fold[fold]= acc(y_test[fold],SVM_Prediction_test)\n",
    "        print(\"y_test:\",y_test[fold],\"SVM:\",SVM_Prediction_test,\"F1:\",F1_test_fold[fold],\"C:\",Parameters_grid['C'])\n",
    "        #F1_train[sub,emo,fold,c]= f1_score(ytrain,SVM_Prediction_train,average='macro')\n",
    "\n",
    "\n",
    "        fold+=1\n",
    "\n",
    "    #---F1-score evaluation for test set---------------------------------------------------------------------------------------\n",
    "\n",
    "    print('INICIO TEST')\n",
    "    #Calcular F1_test\n",
    "    #print(y_test[sub,:],SVM_Prediction_test[sub,:,band,c])\n",
    "    F1_test = np.mean(F1_test_fold[:])\n",
    "    ACC_test= np.mean(ACC_test_fold[:])\n",
    "    STD_F1_test = np.std(F1_test_fold[:])\n",
    "    STD_ACC_test = np.std(ACC_test_fold[:])\n",
    "    print(ACC_test)\n",
    "    \n",
    "    wav= wave_name[wave]+'_valences_rois_easy_MKL'\n",
    "    #Pickle Save F1 Score\n",
    "    path_save= \"Wavelets_Results_ROIs_2021/\"\n",
    "    \n",
    "    import pickle\n",
    "    file= open(path_save+'F1_Score_'+wav+'.pckl', 'wb')\n",
    "    pickle.dump(F1_test,file)\n",
    "    file.close()\n",
    "\n",
    "    File= open(path_save+'F1_Score_'+wav+'.pckl', 'rb')\n",
    "    Load = pickle.load(File)\n",
    "    File.close()\n",
    "    print(Load.shape)\n",
    "    if (Load == F1_test).all:\n",
    "        print(\"Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\")\n",
    "    else:\n",
    "        print(\"Sigue intentando\")\n",
    "\n",
    "    #Pickle Save Accuracy\n",
    "    file= open(path_save+'ACC_Score_'+wav+'.pckl', 'wb')\n",
    "    pickle.dump(ACC_test,file)\n",
    "    file.close()\n",
    "\n",
    "    File= open(path_save+'ACC_Score_'+wav+'.pckl', 'rb')\n",
    "    Load = pickle.load(File)\n",
    "    File.close()\n",
    "    print(Load.shape)\n",
    "    if (Load == ACC_test).all:\n",
    "        print(\"Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\")\n",
    "    else:\n",
    "        print(\"Sigue intentando\")\n",
    "    \n",
    "    #Pickle Save Weights\n",
    "    file= open(path_save+'Weights_'+wav+'.pckl', 'wb')\n",
    "    pickle.dump(weights,file)\n",
    "    file.close()\n",
    "\n",
    "    File= open(path_save+'Weights_'+wav+'.pckl', 'rb')\n",
    "    Load = pickle.load(File)\n",
    "    File.close()\n",
    "    print(Load.shape)\n",
    "    if (Load == weights).all:\n",
    "        print(\"Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\")\n",
    "    else:\n",
    "        print(\"Sigue intentando\")\n",
    "        \n",
    "    #Pickle Save STD\n",
    "    file= open(path_save+'STD_F1_'+wav+'.pckl', 'wb')\n",
    "    pickle.dump(STD_F1_test,file)\n",
    "    file.close()\n",
    "\n",
    "    File= open(path_save+'STD_F1_'+wav+'.pckl', 'rb')\n",
    "    Load = pickle.load(File)\n",
    "    File.close()\n",
    "    print(Load.shape)\n",
    "    if (Load == STD_F1_test).all:\n",
    "        print(\"Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\")\n",
    "    else:\n",
    "        print(\"Sigue intentando\")\n",
    "        \n",
    "        \n",
    "    #Pickle Save STD ACC\n",
    "    file= open(path_save+'STD_ACC_'+wav+'.pckl', 'wb')\n",
    "    pickle.dump(STD_ACC_test,file)\n",
    "    file.close()\n",
    "\n",
    "    File= open(path_save+'STD_ACC_'+wav+'.pckl', 'rb')\n",
    "    Load = pickle.load(File)\n",
    "    File.close()\n",
    "    print(Load.shape)\n",
    "    if (Load == STD_ACC_test).all:\n",
    "        print(\"Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\")\n",
    "    else:\n",
    "        print(\"Sigue intentando\")\n",
    "        \n",
    "    print('SE GUARDÃ“ EL ARCHIVO '+wav) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using 5 bands and no-valences: \n",
    "### Here it was performed a classification mixing the Valences, ROIs and statistical features in a single dimension, keeping the bands dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 50, 192)\n",
      "Analizando Wavelet: sym8\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for axis 0 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m band \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,N_Band):\n\u001b[0;32m     40\u001b[0m     scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n\u001b[1;32m---> 41\u001b[0m     K[band,:,:] \u001b[38;5;241m=\u001b[39m KerRBF(np\u001b[38;5;241m.\u001b[39msqueeze(\u001b[43mFeatures_Stats_Wavelets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mband\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m))\n\u001b[0;32m     42\u001b[0m     K[band,:,:] \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(K[band,:,:])\n\u001b[0;32m     43\u001b[0m     K[band,:,:] \u001b[38;5;241m=\u001b[39m normalize(K[band,:,:])\n",
      "\u001b[1;31mIndexError\u001b[0m: index 4 is out of bounds for axis 0 with size 4"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.model_selection import KFold as KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#---Vars Definition-------------------------------------------------------------------------------------\n",
    "Valences=3\n",
    "CrossVal= KFold(n_splits=25,shuffle=False)\n",
    "N_Band= 5\n",
    "N_Trials= 50\n",
    "#C = 2.**np.arange(-15,16)\n",
    "#SVM_C=Moda(N_Emociones,Subjects,Best_C_Matrix)\n",
    "#N_cs = len(C)\n",
    "\n",
    "\n",
    "#---Iterations for each Emotion and subject--------------------------------------------------------------------------\n",
    "\n",
    "wave_name= ['bior3.3','bior6.8','sym2','sym5','sym8','coif1','coif5','db4','db10','db15']\n",
    "\n",
    "for wave in range(4,5):\n",
    "    File= open('Wavelets_Results_ROIs_2021/Features_Stats_Wavelets_'+wave_name[wave]+'_valencesrois'+'.pckl', 'rb')\n",
    "    Features_Stats_Wavelets = pickle.load(File)\n",
    "    File.close()\n",
    "    N_folds= CrossVal.get_n_splits(np.squeeze(Features_Stats_Wavelets[1,:,:]))\n",
    "    \n",
    "    F1_test_fold = np.zeros([N_folds])\n",
    "    ACC_test_fold = np.zeros([N_folds])\n",
    "    #SVM_Prediction_test=np.zeros([Valences,N_folds])\n",
    "    K = np.zeros([N_Band,N_Trials,N_Trials])\n",
    "    weights=np.zeros([N_folds,N_Band])\n",
    "\n",
    "    print(Features_Stats_Wavelets.shape)\n",
    "    \n",
    "\n",
    "    y_test = []\n",
    "    print('Analizando Wavelet: '+wave_name[wave]) \n",
    "\n",
    "    for band in range(0,N_Band):\n",
    "        scaler = MinMaxScaler()\n",
    "        K[band,:,:] = KerRBF(np.squeeze(Features_Stats_Wavelets[band,:,:]))\n",
    "        K[band,:,:] = scaler.fit_transform(K[band,:,:])\n",
    "        K[band,:,:] = normalize(K[band,:,:])\n",
    "\n",
    "    fold = 0\n",
    "\n",
    "    for train_index, test_index in CrossVal.split(np.squeeze(Features_Stats_Wavelets[1,:,:]),label_matrix):\n",
    "        ytrain, ytest = label_matrix[train_index], label_matrix[test_index]\n",
    "        ytrainMKL, ytestMKL = label_matrix[train_index], label_matrix[test_index]\n",
    "        ytrain2=np.expand_dims(ytrainMKL,axis=1)\n",
    "        y_test.append(ytest)\n",
    "\n",
    "        #Xtrain,Xtest = np.squeeze(Features_Stats_Wavelets[valence,band,train_index,:]), (Features_Stats_Wavelets[valence,band,test_index,:])\n",
    "\n",
    "    #---Definition of kernels variables for the multiple kernel learning--------------------------------------------------------\n",
    "\n",
    "        Ktrain = np.zeros([N_Band,len(train_index),len(train_index)])\n",
    "        Ktrain =  K[:,train_index,:]\n",
    "        Ktrain = Ktrain[:,:,train_index]\n",
    "        Ktest = np.zeros([N_Band,len(test_index),len(train_index)])\n",
    "        Ktest = K[:,test_index,:]\n",
    "        Ktest = Ktest[:,:,train_index]\n",
    "\n",
    "\n",
    "    #--- Multiple Kernel Learning GRAM---------------------------------------------------------------------\n",
    "\n",
    "        #earlystop = EarlyStopping(\n",
    "        #            Ktrain, ytrain,      #validation data, KL is a validation kernels list\n",
    "        #            patience=5,     #max number of acceptable negative steps\n",
    "        #            cooldown=1,     #how ofter we run a measurement, 1= every optimization step\n",
    "        #            metric='auc',   #the metric we monitor\n",
    "        #            )\n",
    "\n",
    "        #scheduler = ReduceOnWorsening()\n",
    "\n",
    "        #mkl = GRAM(\n",
    "        #max_iter=1000,          \n",
    "        #learning_rate=.01,      \n",
    "        #callbacks=[earlystop],\n",
    "        #scheduler=ReduceOnWorsening()).fit(Ktrain, ytrain)\n",
    "        #print(mkl)\n",
    "        #mkl.predict(Ktest)\n",
    "\n",
    "\n",
    "    #---Multiple Kernel Learning Custom Functions---------------------------------------------------------------------------\n",
    "\n",
    "        clf_Linear_Easy = EasyMKL(lam=1).fit(Ktrain,ytrain)\t#a wrapper for averaging kernels\n",
    "        #clf_Linear_Easy = AverageMKL().fit(Ktrain,ytrain)\n",
    "        weights_train = clf_Linear_Easy.weights\n",
    "        #print(weights_train)\n",
    "\n",
    "        #weights_train = ckaweightedK(Ktrain,ytrain)\n",
    "\n",
    "        #weights_train = np.random.uniform(low=0.0, high=1.0, size=N_Band)\n",
    "        #weights_train = weights_train/np.sum(weights_train)\n",
    "\n",
    "        #aux=np.ones(len(weights_train))\n",
    "        #weights_train = aux-weights_train\n",
    "        #print(weights_train)\n",
    "        weights[fold]=weights_train\n",
    "        Kernel_train= MixKer(Ktrain,weights_train)\n",
    "        Kernel_test= MixKer(Ktest,weights_train)\n",
    "\n",
    "    #---Multiple Kernel Learning------------------------------------------------------------------------------------------------\n",
    "        \"\"\"\"\n",
    "        clf_Linear_Average = AverageMKL().fit(Ktrain,ytrain)\t#a wrapper for averaging kernels\n",
    "        #print ('done')\n",
    "        #print(clf_Linear_Average.weights)\t\t\t#print the weights of the combination of base kernels\n",
    "        K_Linear_average = clf_Linear_Average.ker_matrix\t#the combined kernel matrix\n",
    "        ##print(K_average)\n",
    "        #print ('training EasyMKL...', end='')\n",
    "        clf_Linear_Easy = EasyMKL().fit(Ktrain,ytrain)\t#a wrapper for averaging kernels\n",
    "        #print ('done')\n",
    "        #print(clf_Linear_Easy.weights)\t\t\t#print the weights of the combination of base kernels\n",
    "        K_Linear_easy = clf_Linear_Easy.ker_matrix\t#the combined kernel matrix\n",
    "        #print(K_Linear_average)\n",
    "        Ktrain = MixKer(Ktrain,clf_Linear_Easy.weights)\n",
    "        Ktest = MixKer(Ktest,clf_Linear_Easy.weights)\n",
    "        \"\"\"\n",
    "    #---F1-score evaluation for training set------------------------------------------------------------------------------------\n",
    "\n",
    "        #Distances= scipy.spatial.distance.pdist(Xtrain, metric='euclidean')\n",
    "        #Median= statistics.median(Distances)\n",
    "        Parameters_grid = svc_param_selection(ytrain,5,Kernel_train)\n",
    "        SVM_Classifier= svm.SVC(kernel='precomputed',C=Parameters_grid['C'],class_weight = 'balanced')\n",
    "        SVM_Classifier.fit(Kernel_train,ytrain)\n",
    "        SVM_Prediction_train= SVM_Classifier.predict(Kernel_train)\n",
    "        SVM_Prediction_test= SVM_Classifier.predict(Kernel_test)\n",
    "        F1_test_fold[fold] = f1_score(y_test[fold].T[0],SVM_Prediction_test,zero_division=1)\n",
    "        ACC_test_fold[fold]= acc(y_test[fold].T[0],SVM_Prediction_test)\n",
    "        print(\"y_test:\",y_test[fold].T[0],\"SVM:\",SVM_Prediction_test,\"ACC:\",ACC_test_fold[fold],\"F1:\",F1_test_fold[fold],\"C:\",Parameters_grid['C'])\n",
    "        #F1_train[sub,emo,fold,c]= f1_score(ytrain,SVM_Prediction_train,average='macro')\n",
    "\n",
    "\n",
    "        fold+=1\n",
    "\n",
    "        #---F1-score evaluation for test set---------------------------------------------------------------------------------------\n",
    "\n",
    "        print('INICIO TEST')\n",
    "        #Calcular F1_test\n",
    "        #print(y_test[sub,:],SVM_Prediction_test[sub,:,band,c])\n",
    "        F1_test = np.mean(F1_test_fold[:])\n",
    "        ACC_test= np.mean(ACC_test_fold[:])\n",
    "        STD_F1_test = np.std(F1_test_fold[:])\n",
    "        STD_ACC_test = np.std(ACC_test_fold[:])\n",
    "        print(ACC_test)\n",
    "    #graficar(F1_test,Subjects,emocion)\n",
    "    \n",
    "    wav= wave_name[wave]+'_valencesrois_easy_MKL'\n",
    "    #Pickle Save F1 Score\n",
    "    path_save= \"Wavelets_Results_ROIs_2021/\"\n",
    "    \n",
    "    import pickle\n",
    "    file= open(path_save+'F1_Score_'+wav+'.pckl', 'wb')\n",
    "    pickle.dump(F1_test,file)\n",
    "    file.close()\n",
    "\n",
    "    File= open(path_save+'F1_Score_'+wav+'.pckl', 'rb')\n",
    "    Load = pickle.load(File)\n",
    "    File.close()\n",
    "    print(Load.shape)\n",
    "    if (Load == F1_test).all:\n",
    "        print(\"Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\")\n",
    "    else:\n",
    "        print(\"Sigue intentando\")\n",
    "\n",
    "    #Pickle Save Accuracy\n",
    "    file= open(path_save+'ACC_Score_'+wav+'.pckl', 'wb')\n",
    "    pickle.dump(ACC_test,file)\n",
    "    file.close()\n",
    "\n",
    "    File= open(path_save+'ACC_Score_'+wav+'.pckl', 'rb')\n",
    "    Load = pickle.load(File)\n",
    "    File.close()\n",
    "    print(Load.shape)\n",
    "    if (Load == ACC_test).all:\n",
    "        print(\"Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\")\n",
    "    else:\n",
    "        print(\"Sigue intentando\")\n",
    "    \n",
    "    #Pickle Save Weights\n",
    "    file= open(path_save+'Weights_'+wav+'.pckl', 'wb')\n",
    "    pickle.dump(weights,file)\n",
    "    file.close()\n",
    "\n",
    "    File= open(path_save+'Weights_'+wav+'.pckl', 'rb')\n",
    "    Load = pickle.load(File)\n",
    "    File.close()\n",
    "    print(Load.shape)\n",
    "    if (Load == weights).all:\n",
    "        print(\"Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\")\n",
    "    else:\n",
    "        print(\"Sigue intentando\")\n",
    "        \n",
    "    #Pickle Save STD\n",
    "    file= open(path_save+'STD_F1_'+wav+'.pckl', 'wb')\n",
    "    pickle.dump(STD_F1_test,file)\n",
    "    file.close()\n",
    "\n",
    "    File= open(path_save+'STD_F1_'+wav+'.pckl', 'rb')\n",
    "    Load = pickle.load(File)\n",
    "    File.close()\n",
    "    print(Load.shape)\n",
    "    if (Load == STD_F1_test).all:\n",
    "        print(\"Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\")\n",
    "    else:\n",
    "        print(\"Sigue intentando\")\n",
    "        \n",
    "        \n",
    "    #Pickle Save STD ACC\n",
    "    file= open(path_save+'STD_ACC_'+wav+'.pckl', 'wb')\n",
    "    pickle.dump(STD_ACC_test,file)\n",
    "    file.close()\n",
    "\n",
    "    File= open(path_save+'STD_ACC_'+wav+'.pckl', 'rb')\n",
    "    Load = pickle.load(File)\n",
    "    File.close()\n",
    "    print(Load.shape)\n",
    "    if (Load == STD_ACC_test).all:\n",
    "        print(\"Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\")\n",
    "    else:\n",
    "        print(\"Sigue intentando\")\n",
    "        \n",
    "    print('SE GUARDÃ“ EL ARCHIVO '+wav) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using 8 Rois and no-valences: \n",
    "### Here it was performed a classification mixing the Valences, Bands and statistical features in a single dimension, keeping the Rois dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.model_selection import KFold as KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#---Vars Definition-------------------------------------------------------------------------------------\n",
    "Valences=3\n",
    "CrossVal= KFold(n_splits=25,shuffle=False)\n",
    "N_Band= 8\n",
    "N_Trials= 50\n",
    "#C = 2.**np.arange(-15,16)\n",
    "#SVM_C=Moda(N_Emociones,Subjects,Best_C_Matrix)\n",
    "#N_cs = len(C)\n",
    "\n",
    "\n",
    "#---Iterations for each Emotion and subject--------------------------------------------------------------------------\n",
    "\n",
    "wave_name= ['bior3.3','bior6.8','sym2','sym5','sym8','coif1','coif5','db4','db10','db15']\n",
    "\n",
    "for wave in range(4,5):\n",
    "    File= open('Wavelets_Results_ROIs_2021/Features_Stats_Wavelets_'+wave_name[wave]+'_valencesbands'+'.pckl', 'rb')\n",
    "    Features_Stats_Wavelets = pickle.load(File)\n",
    "    File.close()\n",
    "    N_folds= CrossVal.get_n_splits(np.squeeze(Features_Stats_Wavelets[1,:,:]))\n",
    "    \n",
    "    F1_test_fold = np.zeros([N_folds])\n",
    "    ACC_test_fold = np.zeros([N_folds])\n",
    "    #SVM_Prediction_test=np.zeros([Valences,N_folds])\n",
    "    K = np.zeros([N_Band,N_Trials,N_Trials])\n",
    "    weights=np.zeros([N_folds,N_Band])\n",
    "\n",
    "    print(Features_Stats_Wavelets.shape)\n",
    "    \n",
    "\n",
    "    y_test = []\n",
    "    print('Analizando Wavelet: '+wave_name[wave]) \n",
    "\n",
    "    for band in range(0,N_Band):\n",
    "        scaler = MinMaxScaler()\n",
    "        K[band,:,:] = KerRBF(np.squeeze(Features_Stats_Wavelets[band,:,:]))\n",
    "        K[band,:,:] = scaler.fit_transform(K[band,:,:])\n",
    "        K[band,:,:] = normalize(K[band,:,:])\n",
    "\n",
    "    fold = 0\n",
    "\n",
    "    for train_index, test_index in CrossVal.split(np.squeeze(Features_Stats_Wavelets[1,:,:]),label_matrix):\n",
    "        ytrain, ytest = label_matrix[train_index], label_matrix[test_index]\n",
    "        ytrainMKL, ytestMKL = label_matrix[train_index], label_matrix[test_index]\n",
    "        ytrain2=np.expand_dims(ytrainMKL,axis=1)\n",
    "        y_test.append(ytest)\n",
    "\n",
    "        #Xtrain,Xtest = np.squeeze(Features_Stats_Wavelets[valence,band,train_index,:]), (Features_Stats_Wavelets[valence,band,test_index,:])\n",
    "\n",
    "    #---Definition of kernels variables for the multiple kernel learning--------------------------------------------------------\n",
    "\n",
    "        Ktrain = np.zeros([N_Band,len(train_index),len(train_index)])\n",
    "        Ktrain =  K[:,train_index,:]\n",
    "        Ktrain = Ktrain[:,:,train_index]\n",
    "        Ktest = np.zeros([N_Band,len(test_index),len(train_index)])\n",
    "        Ktest = K[:,test_index,:]\n",
    "        Ktest = Ktest[:,:,train_index]\n",
    "\n",
    "\n",
    "    #--- Multiple Kernel Learning GRAM---------------------------------------------------------------------\n",
    "\n",
    "        #earlystop = EarlyStopping(\n",
    "        #            Ktrain, ytrain,      #validation data, KL is a validation kernels list\n",
    "        #            patience=5,     #max number of acceptable negative steps\n",
    "        #            cooldown=1,     #how ofter we run a measurement, 1= every optimization step\n",
    "        #            metric='auc',   #the metric we monitor\n",
    "        #            )\n",
    "\n",
    "        #scheduler = ReduceOnWorsening()\n",
    "\n",
    "        #mkl = GRAM(\n",
    "        #max_iter=1000,          \n",
    "        #learning_rate=.01,      \n",
    "        #callbacks=[earlystop],\n",
    "        #scheduler=ReduceOnWorsening()).fit(Ktrain, ytrain)\n",
    "        #print(mkl)\n",
    "        #mkl.predict(Ktest)\n",
    "\n",
    "\n",
    "    #---Multiple Kernel Learning Custom Functions---------------------------------------------------------------------------\n",
    "\n",
    "        clf_Linear_Easy = EasyMKL(lam=1).fit(Ktrain,ytrain)\t#a wrapper for averaging kernels\n",
    "        #clf_Linear_Easy = AverageMKL().fit(Ktrain,ytrain)\n",
    "        weights_train = clf_Linear_Easy.weights\n",
    "        #print(weights_train)\n",
    "\n",
    "        #weights_train = ckaweightedK(Ktrain,ytrain)\n",
    "\n",
    "        #weights_train = np.random.uniform(low=0.0, high=1.0, size=N_Band)\n",
    "        #weights_train = weights_train/np.sum(weights_train)\n",
    "\n",
    "        #aux=np.ones(len(weights_train))\n",
    "        #weights_train = aux-weights_train\n",
    "        #print(weights_train)\n",
    "        weights[fold]=weights_train\n",
    "        Kernel_train= MixKer(Ktrain,weights_train)\n",
    "        Kernel_test= MixKer(Ktest,weights_train)\n",
    "\n",
    "    #---Multiple Kernel Learning------------------------------------------------------------------------------------------------\n",
    "        \"\"\"\"\n",
    "        clf_Linear_Average = AverageMKL().fit(Ktrain,ytrain)\t#a wrapper for averaging kernels\n",
    "        #print ('done')\n",
    "        #print(clf_Linear_Average.weights)\t\t\t#print the weights of the combination of base kernels\n",
    "        K_Linear_average = clf_Linear_Average.ker_matrix\t#the combined kernel matrix\n",
    "        ##print(K_average)\n",
    "        #print ('training EasyMKL...', end='')\n",
    "        clf_Linear_Easy = EasyMKL().fit(Ktrain,ytrain)\t#a wrapper for averaging kernels\n",
    "        #print ('done')\n",
    "        #print(clf_Linear_Easy.weights)\t\t\t#print the weights of the combination of base kernels\n",
    "        K_Linear_easy = clf_Linear_Easy.ker_matrix\t#the combined kernel matrix\n",
    "        #print(K_Linear_average)\n",
    "        Ktrain = MixKer(Ktrain,clf_Linear_Easy.weights)\n",
    "        Ktest = MixKer(Ktest,clf_Linear_Easy.weights)\n",
    "        \"\"\"\n",
    "    #---F1-score evaluation for training set------------------------------------------------------------------------------------\n",
    "\n",
    "        #Distances= scipy.spatial.distance.pdist(Xtrain, metric='euclidean')\n",
    "        #Median= statistics.median(Distances)\n",
    "        Parameters_grid = svc_param_selection(ytrain,5,Kernel_train)\n",
    "        SVM_Classifier= svm.SVC(kernel='precomputed',C=Parameters_grid['C'],class_weight = 'balanced')\n",
    "        SVM_Classifier.fit(Kernel_train,ytrain)\n",
    "        SVM_Prediction_train= SVM_Classifier.predict(Kernel_train)\n",
    "        SVM_Prediction_test= SVM_Classifier.predict(Kernel_test)\n",
    "        F1_test_fold[fold] = f1_score(y_test[fold].T[0],SVM_Prediction_test,zero_division=1)\n",
    "        ACC_test_fold[fold]= acc(y_test[fold].T[0],SVM_Prediction_test)\n",
    "        print(\"y_test:\",y_test[fold].T[0],\"SVM:\",SVM_Prediction_test,\"ACC:\",ACC_test_fold[fold],\"F1:\",F1_test_fold[fold],\"C:\",Parameters_grid['C'])\n",
    "        #F1_train[sub,emo,fold,c]= f1_score(ytrain,SVM_Prediction_train,average='macro')\n",
    "\n",
    "\n",
    "        fold+=1\n",
    "\n",
    "        #---F1-score evaluation for test set---------------------------------------------------------------------------------------\n",
    "\n",
    "        print('INICIO TEST')\n",
    "        #Calcular F1_test\n",
    "        #print(y_test[sub,:],SVM_Prediction_test[sub,:,band,c])\n",
    "        F1_test = np.mean(F1_test_fold[:])\n",
    "        ACC_test= np.mean(ACC_test_fold[:])\n",
    "        STD_F1_test = np.std(F1_test_fold[:])\n",
    "        STD_ACC_test = np.std(ACC_test_fold[:])\n",
    "        print(ACC_test)\n",
    "    #graficar(F1_test,Subjects,emocion)\n",
    "    \n",
    "    wav= wave_name[wave]+'_valencesbands_easy_MKL'\n",
    "    #Pickle Save F1 Score\n",
    "    path_save= \"Wavelets_Results_ROIs_2021/\"\n",
    "    \n",
    "    import pickle\n",
    "    file= open(path_save+'F1_Score_'+wav+'.pckl', 'wb')\n",
    "    pickle.dump(F1_test,file)\n",
    "    file.close()\n",
    "\n",
    "    File= open(path_save+'F1_Score_'+wav+'.pckl', 'rb')\n",
    "    Load = pickle.load(File)\n",
    "    File.close()\n",
    "    print(Load.shape)\n",
    "    if (Load == F1_test).all:\n",
    "        print(\"Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\")\n",
    "    else:\n",
    "        print(\"Sigue intentando\")\n",
    "\n",
    "    #Pickle Save Accuracy\n",
    "    file= open(path_save+'ACC_Score_'+wav+'.pckl', 'wb')\n",
    "    pickle.dump(ACC_test,file)\n",
    "    file.close()\n",
    "\n",
    "    File= open(path_save+'ACC_Score_'+wav+'.pckl', 'rb')\n",
    "    Load = pickle.load(File)\n",
    "    File.close()\n",
    "    print(Load.shape)\n",
    "    if (Load == ACC_test).all:\n",
    "        print(\"Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\")\n",
    "    else:\n",
    "        print(\"Sigue intentando\")\n",
    "    \n",
    "    #Pickle Save Weights\n",
    "    file= open(path_save+'Weights_'+wav+'.pckl', 'wb')\n",
    "    pickle.dump(weights,file)\n",
    "    file.close()\n",
    "\n",
    "    File= open(path_save+'Weights_'+wav+'.pckl', 'rb')\n",
    "    Load = pickle.load(File)\n",
    "    File.close()\n",
    "    print(Load.shape)\n",
    "    if (Load == weights).all:\n",
    "        print(\"Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\")\n",
    "    else:\n",
    "        print(\"Sigue intentando\")\n",
    "        \n",
    "    #Pickle Save STD\n",
    "    file= open(path_save+'STD_F1_'+wav+'.pckl', 'wb')\n",
    "    pickle.dump(STD_F1_test,file)\n",
    "    file.close()\n",
    "\n",
    "    File= open(path_save+'STD_F1_'+wav+'.pckl', 'rb')\n",
    "    Load = pickle.load(File)\n",
    "    File.close()\n",
    "    print(Load.shape)\n",
    "    if (Load == STD_F1_test).all:\n",
    "        print(\"Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\")\n",
    "    else:\n",
    "        print(\"Sigue intentando\")\n",
    "        \n",
    "        \n",
    "    #Pickle Save STD ACC\n",
    "    file= open(path_save+'STD_ACC_'+wav+'.pckl', 'wb')\n",
    "    pickle.dump(STD_ACC_test,file)\n",
    "    file.close()\n",
    "\n",
    "    File= open(path_save+'STD_ACC_'+wav+'.pckl', 'rb')\n",
    "    Load = pickle.load(File)\n",
    "    File.close()\n",
    "    print(Load.shape)\n",
    "    if (Load == STD_ACC_test).all:\n",
    "        print(\"Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\")\n",
    "    else:\n",
    "        print(\"Sigue intentando\")\n",
    "        \n",
    "    print('SE GUARDÃ“ EL ARCHIVO '+wav) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using 40 bands and no-valences: \n",
    "### Here it was performed a classification mixing the Valences, ROIs and statistical features in a single dimension, keeping the bands dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.model_selection import KFold as KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#---Vars Definition-------------------------------------------------------------------------------------\n",
    "Valences=3\n",
    "CrossVal= KFold(n_splits=25,shuffle=False)\n",
    "N_Band= 40\n",
    "N_Trials= 50\n",
    "#C = 2.**np.arange(-15,16)\n",
    "#SVM_C=Moda(N_Emociones,Subjects,Best_C_Matrix)\n",
    "#N_cs = len(C)\n",
    "\n",
    "\n",
    "#---Iterations for each Emotion and subject--------------------------------------------------------------------------\n",
    "\n",
    "wave_name= ['bior3.3','bior6.8','sym2','sym5','sym8','coif1','coif5','db4','db10','db15']\n",
    "\n",
    "for wave in range(4,5):\n",
    "    File= open('Wavelets_Results_ROIs_2021/Features_Stats_Wavelets_'+wave_name[wave]+'_bandrois'+'.pckl', 'rb')\n",
    "    Features_Stats_Wavelets = pickle.load(File)\n",
    "    File.close()\n",
    "    N_folds= CrossVal.get_n_splits(np.squeeze(Features_Stats_Wavelets[1,:,:]))\n",
    "    \n",
    "    F1_test_fold = np.zeros([N_folds])\n",
    "    ACC_test_fold = np.zeros([N_folds])\n",
    "    #SVM_Prediction_test=np.zeros([Valences,N_folds])\n",
    "    K = np.zeros([N_Band,N_Trials,N_Trials])\n",
    "    weights=np.zeros([N_folds,N_Band])\n",
    "\n",
    "    print(Features_Stats_Wavelets.shape)\n",
    "    \n",
    "\n",
    "    y_test = []\n",
    "    print('Analizando Wavelet: '+wave_name[wave]) \n",
    "\n",
    "    for band in range(0,N_Band):\n",
    "        scaler = MinMaxScaler()\n",
    "        K[band,:,:] = KerRBF(np.squeeze(Features_Stats_Wavelets[band,:,:]))\n",
    "        K[band,:,:] = scaler.fit_transform(K[band,:,:])\n",
    "        K[band,:,:] = normalize(K[band,:,:])\n",
    "\n",
    "    fold = 0\n",
    "\n",
    "    for train_index, test_index in CrossVal.split(np.squeeze(Features_Stats_Wavelets[1,:,:]),label_matrix):\n",
    "        ytrain, ytest = label_matrix[train_index], label_matrix[test_index]\n",
    "        ytrainMKL, ytestMKL = label_matrix[train_index], label_matrix[test_index]\n",
    "        ytrain2=np.expand_dims(ytrainMKL,axis=1)\n",
    "        y_test.append(ytest)\n",
    "\n",
    "        #Xtrain,Xtest = np.squeeze(Features_Stats_Wavelets[valence,band,train_index,:]), (Features_Stats_Wavelets[valence,band,test_index,:])\n",
    "\n",
    "    #---Definition of kernels variables for the multiple kernel learning--------------------------------------------------------\n",
    "\n",
    "        Ktrain = np.zeros([N_Band,len(train_index),len(train_index)])\n",
    "        Ktrain =  K[:,train_index,:]\n",
    "        Ktrain = Ktrain[:,:,train_index]\n",
    "        Ktest = np.zeros([N_Band,len(test_index),len(train_index)])\n",
    "        Ktest = K[:,test_index,:]\n",
    "        Ktest = Ktest[:,:,train_index]\n",
    "\n",
    "\n",
    "    #--- Multiple Kernel Learning GRAM---------------------------------------------------------------------\n",
    "\n",
    "        #earlystop = EarlyStopping(\n",
    "        #            Ktrain, ytrain,      #validation data, KL is a validation kernels list\n",
    "        #            patience=5,     #max number of acceptable negative steps\n",
    "        #            cooldown=1,     #how ofter we run a measurement, 1= every optimization step\n",
    "        #            metric='auc',   #the metric we monitor\n",
    "        #            )\n",
    "\n",
    "        #scheduler = ReduceOnWorsening()\n",
    "\n",
    "        #mkl = GRAM(\n",
    "        #max_iter=1000,          \n",
    "        #learning_rate=.01,      \n",
    "        #callbacks=[earlystop],\n",
    "        #scheduler=ReduceOnWorsening()).fit(Ktrain, ytrain)\n",
    "        #print(mkl)\n",
    "        #mkl.predict(Ktest)\n",
    "\n",
    "\n",
    "    #---Multiple Kernel Learning Custom Functions---------------------------------------------------------------------------\n",
    "\n",
    "        #clf_Linear_Easy = EasyMKL(lam=1).fit(Ktrain,ytrain)\t#a wrapper for averaging kernels\n",
    "        #clf_Linear_Easy = AverageMKL().fit(Ktrain,ytrain)\n",
    "        #weights_train = clf_Linear_Easy.weights\n",
    "        #print(weights_train)\n",
    "\n",
    "        weights_train = ckaweightedK(Ktrain,ytrain)\n",
    "\n",
    "        #weights_train = np.random.uniform(low=0.0, high=1.0, size=N_Band)\n",
    "        #weights_train = weights_train/np.sum(weights_train)\n",
    "\n",
    "        #aux=np.ones(len(weights_train))\n",
    "        #weights_train = aux-weights_train\n",
    "        #print(weights_train)\n",
    "        weights[fold]=weights_train\n",
    "        Kernel_train= MixKer(Ktrain,weights_train)\n",
    "        Kernel_test= MixKer(Ktest,weights_train)\n",
    "\n",
    "    #---Multiple Kernel Learning------------------------------------------------------------------------------------------------\n",
    "        \"\"\"\"\n",
    "        clf_Linear_Average = AverageMKL().fit(Ktrain,ytrain)\t#a wrapper for averaging kernels\n",
    "        #print ('done')\n",
    "        #print(clf_Linear_Average.weights)\t\t\t#print the weights of the combination of base kernels\n",
    "        K_Linear_average = clf_Linear_Average.ker_matrix\t#the combined kernel matrix\n",
    "        ##print(K_average)\n",
    "        #print ('training EasyMKL...', end='')\n",
    "        clf_Linear_Easy = EasyMKL().fit(Ktrain,ytrain)\t#a wrapper for averaging kernels\n",
    "        #print ('done')\n",
    "        #print(clf_Linear_Easy.weights)\t\t\t#print the weights of the combination of base kernels\n",
    "        K_Linear_easy = clf_Linear_Easy.ker_matrix\t#the combined kernel matrix\n",
    "        #print(K_Linear_average)\n",
    "        Ktrain = MixKer(Ktrain,clf_Linear_Easy.weights)\n",
    "        Ktest = MixKer(Ktest,clf_Linear_Easy.weights)\n",
    "        \"\"\"\n",
    "    #---F1-score evaluation for training set------------------------------------------------------------------------------------\n",
    "\n",
    "        #Distances= scipy.spatial.distance.pdist(Xtrain, metric='euclidean')\n",
    "        #Median= statistics.median(Distances)\n",
    "        Parameters_grid = svc_param_selection(ytrain,5,Kernel_train)\n",
    "        SVM_Classifier= svm.SVC(kernel='precomputed',C=Parameters_grid['C'],class_weight = 'balanced')\n",
    "        SVM_Classifier.fit(Kernel_train,ytrain)\n",
    "        SVM_Prediction_train= SVM_Classifier.predict(Kernel_train)\n",
    "        SVM_Prediction_test= SVM_Classifier.predict(Kernel_test)\n",
    "        F1_test_fold[fold] = f1_score(y_test[fold].T[0],SVM_Prediction_test,zero_division=1)\n",
    "        ACC_test_fold[fold]= acc(y_test[fold].T[0],SVM_Prediction_test)\n",
    "        print(\"y_test:\",y_test[fold].T[0],\"SVM:\",SVM_Prediction_test,\"ACC:\",ACC_test_fold[fold],\"F1:\",F1_test_fold[fold],\"C:\",Parameters_grid['C'])\n",
    "        #F1_train[sub,emo,fold,c]= f1_score(ytrain,SVM_Prediction_train,average='macro')\n",
    "\n",
    "\n",
    "        fold+=1\n",
    "\n",
    "        #---F1-score evaluation for test set---------------------------------------------------------------------------------------\n",
    "\n",
    "        print('INICIO TEST')\n",
    "        #Calcular F1_test\n",
    "        #print(y_test[sub,:],SVM_Prediction_test[sub,:,band,c])\n",
    "        F1_test = np.mean(F1_test_fold[:])\n",
    "        ACC_test= np.mean(ACC_test_fold[:])\n",
    "        STD_F1_test = np.std(F1_test_fold[:])\n",
    "        STD_ACC_test = np.std(ACC_test_fold[:])\n",
    "        print(ACC_test)\n",
    "    #graficar(F1_test,Subjects,emocion)\n",
    "    \n",
    "    wav= wave_name[wave]+'_bandrois_easy_MKL'\n",
    "    #Pickle Save F1 Score\n",
    "    path_save= \"Wavelets_Results_ROIs_2021/\"\n",
    "    \n",
    "    import pickle\n",
    "    file= open(path_save+'F1_Score_'+wav+'.pckl', 'wb')\n",
    "    pickle.dump(F1_test,file)\n",
    "    file.close()\n",
    "\n",
    "    File= open(path_save+'F1_Score_'+wav+'.pckl', 'rb')\n",
    "    Load = pickle.load(File)\n",
    "    File.close()\n",
    "    print(Load.shape)\n",
    "    if (Load == F1_test).all:\n",
    "        print(\"Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\")\n",
    "    else:\n",
    "        print(\"Sigue intentando\")\n",
    "\n",
    "    #Pickle Save Accuracy\n",
    "    file= open(path_save+'ACC_Score_'+wav+'.pckl', 'wb')\n",
    "    pickle.dump(ACC_test,file)\n",
    "    file.close()\n",
    "\n",
    "    File= open(path_save+'ACC_Score_'+wav+'.pckl', 'rb')\n",
    "    Load = pickle.load(File)\n",
    "    File.close()\n",
    "    print(Load.shape)\n",
    "    if (Load == ACC_test).all:\n",
    "        print(\"Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\")\n",
    "    else:\n",
    "        print(\"Sigue intentando\")\n",
    "    \n",
    "    #Pickle Save Weights\n",
    "    file= open(path_save+'Weights_'+wav+'.pckl', 'wb')\n",
    "    pickle.dump(weights,file)\n",
    "    file.close()\n",
    "\n",
    "    File= open(path_save+'Weights_'+wav+'.pckl', 'rb')\n",
    "    Load = pickle.load(File)\n",
    "    File.close()\n",
    "    print(Load.shape)\n",
    "    if (Load == weights).all:\n",
    "        print(\"Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\")\n",
    "    else:\n",
    "        print(\"Sigue intentando\")\n",
    "        \n",
    "    #Pickle Save STD\n",
    "    file= open(path_save+'STD_F1_'+wav+'.pckl', 'wb')\n",
    "    pickle.dump(STD_F1_test,file)\n",
    "    file.close()\n",
    "\n",
    "    File= open(path_save+'STD_F1_'+wav+'.pckl', 'rb')\n",
    "    Load = pickle.load(File)\n",
    "    File.close()\n",
    "    print(Load.shape)\n",
    "    if (Load == STD_F1_test).all:\n",
    "        print(\"Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\")\n",
    "    else:\n",
    "        print(\"Sigue intentando\")\n",
    "        \n",
    "        \n",
    "    #Pickle Save STD ACC\n",
    "    file= open(path_save+'STD_ACC_'+wav+'.pckl', 'wb')\n",
    "    pickle.dump(STD_ACC_test,file)\n",
    "    file.close()\n",
    "\n",
    "    File= open(path_save+'STD_ACC_'+wav+'.pckl', 'rb')\n",
    "    Load = pickle.load(File)\n",
    "    File.close()\n",
    "    print(Load.shape)\n",
    "    if (Load == STD_ACC_test).all:\n",
    "        print(\"Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\")\n",
    "    else:\n",
    "        print(\"Sigue intentando\")\n",
    "        \n",
    "    print('SE GUARDÃ“ EL ARCHIVO '+wav) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features_Stats_Wavelets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ACC_test)\n",
    "print(STD_F1_test)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNUSED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#---Vars Definition-------------------------------------------------------------------------------------\n",
    "CrossVal= LeaveOneOut()\n",
    "N_Band= 5\n",
    "N_Trials= 50\n",
    "N_Rois = 8\n",
    "C = 2.**np.arange(-15,16)\n",
    "#SVM_C=Moda(N_Emociones,Subjects,Best_C_Matrix)\n",
    "N_cs = len(C)\n",
    "\n",
    "\n",
    "#---Iterations for each Emotion and subject--------------------------------------------------------------------------\n",
    "\n",
    "wave_name= ['bior3.3','bior6.8','sym2','sym5','sym8','coif1','coif5','db4','db10','db15']\n",
    "\n",
    "for wave in range(4,5):\n",
    "    File= open('Wavelets_Results_ROIs/Features_Stats_Wavelets_'+wave_name[wave]+'_valencesrois'+'.pckl', 'rb')\n",
    "    Features_Stats_Wavelets = pickle.load(File)\n",
    "    File.close()\n",
    "    N_folds= CrossVal.get_n_splits(np.squeeze(Features_Stats_Wavelets[1,:,:]))\n",
    "    \n",
    "    \n",
    "    SVM_Prediction_test=np.zeros([N_folds])\n",
    "    y_test = np.zeros([N_folds])\n",
    "    K = np.zeros([N_Band,N_Trials,N_Trials])\n",
    "    weights=np.zeros([N_folds,N_Band])\n",
    "\n",
    "    print(Features_Stats_Wavelets.shape)\n",
    "  \n",
    "    print('Analizando Wavelet: '+wave_name[wave]+' Valencia : ') \n",
    "\n",
    "    for band in range(0,N_Band):\n",
    "        K[band,:,:] = KerRBF(np.squeeze(Features_Stats_Wavelets[band,:,:]))\n",
    "\n",
    "        fold = 0\n",
    "\n",
    "    for train_index, test_index in CrossVal.split(np.squeeze(Features_Stats_Wavelets[1,:,:])):\n",
    "        ytrain, ytest = label_matrix[train_index], label_matrix[test_index]\n",
    "        y_test[fold] = ytest\n",
    "        ytrainMKL, ytestMKL = label_matrix[train_index], label_matrix[test_index]\n",
    "        ytrain2=np.expand_dims(ytrainMKL,axis=1)\n",
    "        y_test[fold] = ytest\n",
    "\n",
    "        Xtrain,Xtest = np.squeeze(Features_Stats_Wavelets[band,train_index,:]), (Features_Stats_Wavelets[band,test_index,:])\n",
    "\n",
    "    #---Definition of kernels variables for the multiple kernel learning--------------------------------------------------------\n",
    "\n",
    "        Ktrain = np.zeros([N_Band,len(train_index),len(train_index)])\n",
    "        Ktrain =  K[:,train_index,:]\n",
    "        Ktrain = Ktrain[:,:,train_index]\n",
    "        Ktest = np.zeros([N_Band,len(test_index),len(train_index)])\n",
    "        Ktest = K[:,test_index,:]\n",
    "        Ktest = Ktest[:,:,train_index]\n",
    "\n",
    "\n",
    "    #--- Multiple Kernel Learning GRAM---------------------------------------------------------------------\n",
    "\n",
    "        #earlystop = EarlyStopping(\n",
    "        #            Ktrain, ytrain,      #validation data, KL is a validation kernels list\n",
    "        #            patience=5,     #max number of acceptable negative steps\n",
    "        #            cooldown=1,     #how ofter we run a measurement, 1= every optimization step\n",
    "        #            metric='auc',   #the metric we monitor\n",
    "        #            )\n",
    "\n",
    "        #scheduler = ReduceOnWorsening()\n",
    "\n",
    "        #mkl = GRAM(\n",
    "        #max_iter=1000,          \n",
    "        #learning_rate=.01,      \n",
    "        #callbacks=[earlystop],\n",
    "        #scheduler=ReduceOnWorsening()).fit(Ktrain, ytrain)\n",
    "        #print(mkl)\n",
    "        #mkl.predict(Ktest)\n",
    "\n",
    "\n",
    "    #---Multiple Kernel Learning Custom Functions---------------------------------------------------------------------------\n",
    "\n",
    "        clf_Linear_Easy = EasyMKL(lam=1).fit(Ktrain,ytrain)\t#a wrapper for averaging kernels\n",
    "        #clf_Linear_Easy = AverageMKL().fit(Ktrain,ytrain)\n",
    "        weights_train = clf_Linear_Easy.weights\n",
    "        #print(weights_train)\n",
    "\n",
    "        #weights_train = ckaweightedK(Ktrain,ytrain)\n",
    "\n",
    "        #weights_train = np.random.uniform(low=0.0, high=1.0, size=N_Band)\n",
    "        #weights_train = weights_train/np.sum(weights_train)\n",
    "\n",
    "        #aux=np.ones(len(weights_train))\n",
    "        #weights_train = aux-weights_train\n",
    "        #print(weights_train)\n",
    "        weights[fold]=weights_train\n",
    "        Kernel_train= MixKer(Ktrain,weights_train)\n",
    "        Kernel_test= MixKer(Ktest,weights_train)\n",
    "\n",
    "    #---Multiple Kernel Learning------------------------------------------------------------------------------------------------\n",
    "        \"\"\"\"\n",
    "        clf_Linear_Average = AverageMKL().fit(Ktrain,ytrain)\t#a wrapper for averaging kernels\n",
    "        #print ('done')\n",
    "        #print(clf_Linear_Average.weights)\t\t\t#print the weights of the combination of base kernels\n",
    "        K_Linear_average = clf_Linear_Average.ker_matrix\t#the combined kernel matrix\n",
    "        ##print(K_average)\n",
    "        #print ('training EasyMKL...', end='')\n",
    "        clf_Linear_Easy = EasyMKL().fit(Ktrain,ytrain)\t#a wrapper for averaging kernels\n",
    "        #print ('done')\n",
    "        #print(clf_Linear_Easy.weights)\t\t\t#print the weights of the combination of base kernels\n",
    "        K_Linear_easy = clf_Linear_Easy.ker_matrix\t#the combined kernel matrix\n",
    "        #print(K_Linear_average)\n",
    "        Ktrain = MixKer(Ktrain,clf_Linear_Easy.weights)\n",
    "        Ktest = MixKer(Ktest,clf_Linear_Easy.weights)\n",
    "        \"\"\"\n",
    "    #---F1-score evaluation for training set------------------------------------------------------------------------------------\n",
    "\n",
    "        Distances= scipy.spatial.distance.pdist(Xtrain, metric='euclidean')\n",
    "        Median= statistics.median(Distances)\n",
    "        #Gamma= 1/(2*(Median**2))\n",
    "        Gamma= 1/(Xtrain.shape[1]*Xtrain.var())\n",
    "        Parameters_grid = svc_param_selection(ytrain,5,Gamma,Kernel_train)\n",
    "        SVM_Classifier= svm.SVC(kernel='precomputed',gamma=Gamma,C=Parameters_grid['C'],class_weight = 'balanced')\n",
    "        SVM_Classifier.fit(Kernel_train,ytrain)\n",
    "        SVM_Prediction_train= SVM_Classifier.predict(Kernel_train)\n",
    "        SVM_Prediction_test[fold]= SVM_Classifier.predict(Kernel_test)\n",
    "        #F1_train[sub,emo,fold,c]= f1_score(ytrain,SVM_Prediction_train,average='macro')\n",
    "\n",
    "\n",
    "        fold+=1\n",
    "\n",
    "        #---F1-score evaluation for test set---------------------------------------------------------------------------------------\n",
    "\n",
    "    print('INICIO TEST')\n",
    "    #Calcular F1_test\n",
    "    #print(y_test[sub,:],SVM_Prediction_test[sub,:,band,c])\n",
    "    F1_test = f1_score(y_test[:],SVM_Prediction_test[:])\n",
    "    ACC_test= acc(y_test[:],SVM_Prediction_test[:])\n",
    "    print(F1_test)\n",
    "    #graficar(F1_test,Subjects,emocion)\n",
    "    \n",
    "    wav= wave_name[wave]+'_valencesrois_easy'\n",
    "    #Pickle Save F1 Score\n",
    "    import pickle\n",
    "    file= open('Wavelets_Results_ROIs/F1_Score_'+wav+'_MKL.pckl', 'wb')\n",
    "    pickle.dump(F1_test,file)\n",
    "    file.close()\n",
    "\n",
    "    File= open('Wavelets_Results_ROIs/F1_Score_'+wav+'_MKL.pckl', 'rb')\n",
    "    Load = pickle.load(File)\n",
    "    File.close()\n",
    "    print(Load.shape)\n",
    "    if (Load == F1_test).all:\n",
    "        print(\"Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\")\n",
    "    else:\n",
    "        print(\"Sigue intentando\")\n",
    "\n",
    "    #Pickle Save Accuracy\n",
    "    file= open('Wavelets_Results_ROIs/ACC_Score_'+wav+'_MKL.pckl', 'wb')\n",
    "    pickle.dump(ACC_test,file)\n",
    "    file.close()\n",
    "\n",
    "    File= open('Wavelets_Results_ROIs/ACC_Score_'+wav+'_MKL.pckl', 'rb')\n",
    "    Load = pickle.load(File)\n",
    "    File.close()\n",
    "    print(Load.shape)\n",
    "    if (Load == ACC_test).all:\n",
    "        print(\"Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\")\n",
    "    else:\n",
    "        print(\"Sigue intentando\")\n",
    "    \n",
    "    #Pickle Save Weights\n",
    "    file= open('Wavelets_Results_ROIs/Weights_'+wav+'_MKL.pckl', 'wb')\n",
    "    pickle.dump(weights,file)\n",
    "    file.close()\n",
    "\n",
    "    File= open('Wavelets_Results_ROIs/Weights_'+wav+'_MKL.pckl', 'rb')\n",
    "    Load = pickle.load(File)\n",
    "    File.close()\n",
    "    print(Load.shape)\n",
    "    if (Load == weights).all:\n",
    "        print(\"Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\")\n",
    "    else:\n",
    "        print(\"Sigue intentando\")\n",
    "        \n",
    "    print('SE GUARDÃ“ EL ARCHIVO '+wav) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#---Vars Definition-------------------------------------------------------------------------------------\n",
    "CrossVal= LeaveOneOut()\n",
    "N_Band= 40\n",
    "N_Trials= 50\n",
    "C = 2.**np.arange(-15,16)\n",
    "#SVM_C=Moda(N_Emociones,Subjects,Best_C_Matrix)\n",
    "N_cs = len(C)\n",
    "\n",
    "\n",
    "#---Iterations for each Emotion and subject--------------------------------------------------------------------------\n",
    "\n",
    "wave_name= ['bior3.3','bior6.8','sym2','sym5','sym8','coif1','coif5','db4','db10','db15']\n",
    "\n",
    "for wave in range(4,5):\n",
    "    File= open('Wavelets_Results_ROIs/Features_Stats_Wavelets_'+wave_name[wave]+'_bandrois'+'.pckl', 'rb')\n",
    "    Features_Stats_Wavelets = pickle.load(File)\n",
    "    File.close()\n",
    "    N_folds= CrossVal.get_n_splits(np.squeeze(Features_Stats_Wavelets[1,:,:]))\n",
    "    \n",
    "    SVM_Prediction_test=np.zeros([N_folds])\n",
    "    y_test = np.zeros([N_folds])\n",
    "    K = np.zeros([N_Band,N_Trials,N_Trials])\n",
    "    weights=np.zeros([N_folds,N_Band])\n",
    "\n",
    "    print(Features_Stats_Wavelets.shape)\n",
    "    \n",
    "   \n",
    "    print('Analizando Wavelet: '+wave_name[wave]) \n",
    "\n",
    "    for band in range(0,N_Band):\n",
    "        K[band,:,:] = KerRBF(np.squeeze(Features_Stats_Wavelets[band,:,:]))\n",
    "\n",
    "        fold = 0\n",
    "\n",
    "    for train_index, test_index in CrossVal.split(np.squeeze(Features_Stats_Wavelets[1,:,:])):\n",
    "        ytrain, ytest = label_matrix[train_index], label_matrix[test_index]\n",
    "        y_test[fold] = ytest\n",
    "        ytrainMKL, ytestMKL = label_matrix[train_index], label_matrix[test_index]\n",
    "        ytrain2=np.expand_dims(ytrainMKL,axis=1)\n",
    "        y_test[fold] = ytest\n",
    "\n",
    "        Xtrain,Xtest = np.squeeze(Features_Stats_Wavelets[band,train_index,:]), (Features_Stats_Wavelets[band,test_index,:])\n",
    "\n",
    "    #---Definition of kernels variables for the multiple kernel learning--------------------------------------------------------\n",
    "\n",
    "        Ktrain = np.zeros([N_Band,len(train_index),len(train_index)])\n",
    "        Ktrain =  K[:,train_index,:]\n",
    "        Ktrain = Ktrain[:,:,train_index]\n",
    "        Ktest = np.zeros([N_Band,len(test_index),len(train_index)])\n",
    "        Ktest = K[:,test_index,:]\n",
    "        Ktest = Ktest[:,:,train_index]\n",
    "\n",
    "\n",
    "    #--- Multiple Kernel Learning GRAM---------------------------------------------------------------------\n",
    "\n",
    "        #earlystop = EarlyStopping(\n",
    "        #            Ktrain, ytrain,      #validation data, KL is a validation kernels list\n",
    "        #            patience=5,     #max number of acceptable negative steps\n",
    "        #            cooldown=1,     #how ofter we run a measurement, 1= every optimization step\n",
    "        #            metric='auc',   #the metric we monitor\n",
    "        #            )\n",
    "\n",
    "        #scheduler = ReduceOnWorsening()\n",
    "\n",
    "        #mkl = GRAM(\n",
    "        #max_iter=1000,          \n",
    "        #learning_rate=.01,      \n",
    "        #callbacks=[earlystop],\n",
    "        #scheduler=ReduceOnWorsening()).fit(Ktrain, ytrain)\n",
    "        #print(mkl)\n",
    "        #mkl.predict(Ktest)\n",
    "\n",
    "\n",
    "    #---Multiple Kernel Learning Custom Functions---------------------------------------------------------------------------\n",
    "\n",
    "        clf_Linear_Easy = EasyMKL(lam=1).fit(Ktrain,ytrain)\t#a wrapper for averaging kernels\n",
    "        #clf_Linear_Easy = AverageMKL().fit(Ktrain,ytrain)\n",
    "        weights_train = clf_Linear_Easy.weights\n",
    "        #print(weights_train)\n",
    "\n",
    "        #weights_train = ckaweightedK(Ktrain,ytrain)\n",
    "\n",
    "        #weights_train = np.random.uniform(low=0.0, high=1.0, size=N_Band)\n",
    "        #weights_train = weights_train/np.sum(weights_train)\n",
    "\n",
    "        #aux=np.ones(len(weights_train))\n",
    "        #weights_train = aux-weights_train\n",
    "        #print(weights_train)\n",
    "        weights[fold]=weights_train\n",
    "        Kernel_train= MixKer(Ktrain,weights_train)\n",
    "        Kernel_test= MixKer(Ktest,weights_train)\n",
    "\n",
    "    #---Multiple Kernel Learning------------------------------------------------------------------------------------------------\n",
    "        \"\"\"\"\n",
    "        clf_Linear_Average = AverageMKL().fit(Ktrain,ytrain)\t#a wrapper for averaging kernels\n",
    "        #print ('done')\n",
    "        #print(clf_Linear_Average.weights)\t\t\t#print the weights of the combination of base kernels\n",
    "        K_Linear_average = clf_Linear_Average.ker_matrix\t#the combined kernel matrix\n",
    "        ##print(K_average)\n",
    "        #print ('training EasyMKL...', end='')\n",
    "        clf_Linear_Easy = EasyMKL().fit(Ktrain,ytrain)\t#a wrapper for averaging kernels\n",
    "        #print ('done')\n",
    "        #print(clf_Linear_Easy.weights)\t\t\t#print the weights of the combination of base kernels\n",
    "        K_Linear_easy = clf_Linear_Easy.ker_matrix\t#the combined kernel matrix\n",
    "        #print(K_Linear_average)\n",
    "        Ktrain = MixKer(Ktrain,clf_Linear_Easy.weights)\n",
    "        Ktest = MixKer(Ktest,clf_Linear_Easy.weights)\n",
    "        \"\"\"\n",
    "    #---F1-score evaluation for training set------------------------------------------------------------------------------------\n",
    "\n",
    "        Distances= scipy.spatial.distance.pdist(Xtrain, metric='euclidean')\n",
    "        Median= statistics.median(Distances)\n",
    "        Parameters_grid = svc_param_selection(ytrain,5,Gamma,Kernel_train)\n",
    "        SVM_Classifier= svm.SVC(kernel='precomputed',gamma=Gamma,C=Parameters_grid['C'],class_weight = 'balanced')\n",
    "        SVM_Classifier.fit(Kernel_train,ytrain)\n",
    "        SVM_Prediction_train= SVM_Classifier.predict(Kernel_train)\n",
    "        SVM_Prediction_test[fold]= SVM_Classifier.predict(Kernel_test)\n",
    "        #F1_train[sub,emo,fold,c]= f1_score(ytrain,SVM_Prediction_train,average='macro')\n",
    "\n",
    "\n",
    "        fold+=1\n",
    "\n",
    "    #---F1-score evaluation for test set---------------------------------------------------------------------------------------\n",
    "\n",
    "    print('INICIO TEST')\n",
    "    #Calcular F1_test\n",
    "    #print(y_test[sub,:],SVM_Prediction_test[sub,:,band,c])\n",
    "    F1_test = f1_score(y_test[:],SVM_Prediction_test[:])\n",
    "    ACC_test = acc(y_test[:],SVM_Prediction_test[:])\n",
    "    print(F1_test)\n",
    "#graficar(F1_test,Subjects,emocion)\n",
    "    \n",
    "    wav= wave_name[wave]+'_bandrois_easy'\n",
    "    #Pickle Save F1 Score\n",
    "    import pickle\n",
    "    file= open('Wavelets_Results_ROIs/F1_Score_'+wav+'_MKL.pckl', 'wb')\n",
    "    pickle.dump(F1_test,file)\n",
    "    file.close()\n",
    "\n",
    "    File= open('Wavelets_Results_ROIs/F1_Score_'+wav+'_MKL.pckl', 'rb')\n",
    "    Load = pickle.load(File)\n",
    "    File.close()\n",
    "    print(Load.shape)\n",
    "    if (Load == F1_test).all:\n",
    "        print(\"Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\")\n",
    "    else:\n",
    "        print(\"Sigue intentando\")\n",
    "\n",
    "    #Pickle Save Accuracy\n",
    "    file= open('Wavelets_Results_ROIs/ACC_Score_'+wav+'_MKL.pckl', 'wb')\n",
    "    pickle.dump(ACC_test,file)\n",
    "    file.close()\n",
    "\n",
    "    File= open('Wavelets_Results_ROIs/ACC_Score_'+wav+'_MKL.pckl', 'rb')\n",
    "    Load = pickle.load(File)\n",
    "    File.close()\n",
    "    print(Load.shape)\n",
    "    if (Load == ACC_test).all:\n",
    "        print(\"Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\")\n",
    "    else:\n",
    "        print(\"Sigue intentando\")\n",
    "    \n",
    "    #Pickle Save Weights\n",
    "    file= open('Wavelets_Results_ROIs/Weights_'+wav+'_MKL.pckl', 'wb')\n",
    "    pickle.dump(weights,file)\n",
    "    file.close()\n",
    "\n",
    "    File= open('Wavelets_Results_ROIs/Weights_'+wav+'_MKL.pckl', 'rb')\n",
    "    Load = pickle.load(File)\n",
    "    File.close()\n",
    "    print(Load.shape)\n",
    "    if (Load == weights).all:\n",
    "        print(\"Excelente ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\")\n",
    "    else:\n",
    "        print(\"Sigue intentando\")\n",
    "        \n",
    "    print('SE GUARDÃ“ EL ARCHIVO '+wav) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "File= open('Wavelets_Results/F1_Score_'+'bior3.3_sparse'+'_MKL.pckl', 'rb')\n",
    "F1_test = pickle.load(File)\n",
    "File.close()\n",
    "print(F1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#F1_test.tofile('Test_5\\F1_Test_KRBF_C3_MKL_Improved'+'.txt')\n",
    "#Best_C_Matrix.tofile('Test_5\\Best_C_Matrix_C2_MKL'+'.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRÃFICA DEL F1_SCORE PARA CADA SUJETO EN CADA BANDA Y PARA CADA C \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#---Vars Definition-------------------------------------------------------------------------------------\n",
    "Subjects= 32\n",
    "CrossVal= LeaveOneOut()\n",
    "N_folds= CrossVal.get_n_splits(np.squeeze(Features_Stats_Wavelets[1,1,:,:]))\n",
    "N_Band= 6\n",
    "N_Trials= 40\n",
    "N_Emociones=4\n",
    "C = 2.**np.arange(-15,19)\n",
    "N_cs = len(C)\n",
    "Best_C_Matrix=np.zeros([Subjects,N_Emociones,N_folds])\n",
    "F1_train = np.zeros([Subjects,N_Emociones,N_folds,N_cs])\n",
    "F1_test = np.zeros([Subjects,N_Emociones])\n",
    "SVM_Prediction_test=np.zeros([Subjects,N_Emociones,N_folds])\n",
    "y_test = np.zeros([Subjects,N_folds])\n",
    "K = np.zeros([N_Band,N_Trials,N_Trials])\n",
    "\n",
    "#---Iterations for each Emotion and subject--------------------------------------------------------------------------\n",
    "\n",
    "for emo in range(N_Emociones):\n",
    "    for sub in range(Subjects):\n",
    "        print('AnÃ¡lisis Sujeto: ',sub, 'EmociÃ³n: ',emo)\n",
    "        if (sub==26 and emo==2):\n",
    "            sub+=1\n",
    "            \n",
    "        for band in range(0,N_Band):\n",
    "            K[band,:,:] = KerRBF(np.squeeze(Features_Stats_Wavelets[sub,band,:,:]))\n",
    "\n",
    "        fold = 0\n",
    "        emocion=emo\n",
    "        for train_index, test_index in CrossVal.split(np.squeeze(Features_Stats_Wavelets[sub,1,:,:])):\n",
    "            ytrain, ytest = label_threshold_matrix_5[sub,train_index,emocion], label_threshold_matrix_5[sub,test_index,emocion]\n",
    "            y_test[sub,fold] = ytest\n",
    "            ytrainMKL, ytestMKL = label_matrix[sub,train_index,emocion], label_matrix[sub,test_index,emocion]\n",
    "            ytrain2=np.expand_dims(ytrainMKL,axis=1)\n",
    "            y_test[sub,fold] = ytest\n",
    "\n",
    "            Xtrain,Xtest = np.squeeze(Features_Stats_Wavelets[sub,band,train_index,:]), (Features_Stats_Wavelets[sub,band,test_index,:])\n",
    "\n",
    "        #---Definition of kernels variables for the multiple kernel learning--------------------------------------------------------\n",
    "\n",
    "            Ktrain = np.zeros([N_Band,len(train_index),len(train_index)])\n",
    "            Ktrain =  K[:,train_index,:]\n",
    "            Ktrain = Ktrain[:,:,train_index]\n",
    "            Ktest = np.zeros([N_Band,len(test_index),len(train_index)])\n",
    "            Ktest = K[:,test_index,:]\n",
    "            Ktest = Ktest[:,:,train_index]\n",
    "\n",
    "        #---Multiple Kernel Learning Custom Functions---------------------------------------------------------------------------\n",
    "\n",
    "            #print(Ktrain.shape, ytrain2.shape)\n",
    "            weights_train = ckaweightedK(Ktrain,ytrain2)\n",
    "            #print(weights_train)\n",
    "            Kernel_train= MixKer(Ktrain,weights_train)\n",
    "            Kernel_test= MixKer(Ktest,weights_train)\n",
    "\n",
    "        #---Multiple Kernel Learning------------------------------------------------------------------------------------------------\n",
    "            \"\"\"\"\n",
    "            clf_Linear_Average = AverageMKL().fit(Ktrain,ytrain)\t#a wrapper for averaging kernels\n",
    "            #print ('done')\n",
    "            #print(clf_Linear_Average.weights)\t\t\t#print the weights of the combination of base kernels\n",
    "            K_Linear_average = clf_Linear_Average.ker_matrix\t#the combined kernel matrix\n",
    "            ##print(K_average)\n",
    "            #print ('training EasyMKL...', end='')\n",
    "            clf_Linear_Easy = EasyMKL().fit(Ktrain,ytrain)\t#a wrapper for averaging kernels\n",
    "            #print ('done')\n",
    "            #print(clf_Linear_Easy.weights)\t\t\t#print the weights of the combination of base kernels\n",
    "            K_Linear_easy = clf_Linear_Easy.ker_matrix\t#the combined kernel matrix\n",
    "            #print(K_Linear_average)\n",
    "            Ktrain = MixKer(Ktrain,clf_Linear_Easy.weights)\n",
    "            Ktest = MixKer(Ktest,clf_Linear_Easy.weights)\n",
    "            \"\"\"\n",
    "        #---F1-score evaluation for training set------------------------------------------------------------------------------------\n",
    "\n",
    "            Best_C= -10000\n",
    "            Best_F1=0\n",
    "            for c in range(len(C)):\n",
    "                Distances= scipy.spatial.distance.pdist(Xtrain, metric='euclidean')\n",
    "                Median= statistics.median(Distances)\n",
    "                Gamma= 1/(Xtrain.shape[1]*Xtrain.var())\n",
    "                SVM_Classifier= svm.SVC(kernel='precomputed',C=C[c])\n",
    "                SVM_Classifier.fit(Kernel_train,ytrain)\n",
    "                SVM_Prediction_train= SVM_Classifier.predict(Kernel_train)\n",
    "                SVM_Prediction_test[sub,emo,fold]= SVM_Classifier.predict(Kernel_test)\n",
    "                F1_train[sub,emo,fold,c]= f1_score(ytrain,SVM_Prediction_train,average='macro')\n",
    "\n",
    "                if F1_train[sub,emo,fold,c]>Best_F1:\n",
    "                    Best_C_Matrix[valence,fold]=C[c]\n",
    "                    Best_C= C[c]\n",
    "                    Best_F1=F1_train[valence,fold,c]\n",
    "            fold+=1\n",
    "\n",
    "\n",
    "\n",
    "        #---F1-score evaluation for test set---------------------------------------------------------------------------------------\n",
    "\n",
    "        print('INICIO TEST')\n",
    "        #Calcular F1_test\n",
    "        #print(y_test[sub,:],SVM_Prediction_test[sub,:,band,c])\n",
    "        F1_test[sub,emo] = f1_score(y_test[sub,:],SVM_Prediction_test[sub,emo,:],average='macro')\n",
    "        #print(F1_test[sub,band,c])\n",
    "        #time.sleep(4)\n",
    "    graficar(F1_test,Subjects,emocion)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "graficar(F1_test,Subjects,3)\n",
    "Moda(N_Emociones,Subjects,Best_C_Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#F1_test.tofile('Test\\F1_Test_Customlib_RBF_Lineal_C3_'+Emocion[emocion]+'.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "band=['Theta','Low Alpha', 'High Alpha', 'Alha', 'Betha']\n",
    "\n",
    "\n",
    "for i in range(0,7):\n",
    "    plt.figure(i)\n",
    "    #plt.title('Kernel for '+ band[i])\n",
    "    plt.imshow(Ktrain[i],cmap= plt.cm.get_cmap('OrRd'))\n",
    "    #plt.savefig('DEAP_Figures/Kernel_'+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Kernel_train,cmap= plt.cm.get_cmap('OrRd'))\n",
    "#plt.savefig('DEAP_Figures/Final_Kernel')\n",
    "print(weights_train) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensayo=np.matmul(ytrain2,ytrain2.T)\n",
    "plt.imshow(ensayo,cmap= plt.cm.get_cmap('OrRd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "x=np.arange(0,496)\n",
    "y=np.arange(0,40)\n",
    "X,Y= np.meshgrid(x,y)\n",
    "z=Y*X\n",
    "print(Feature_MSC_Matrix_C2[0,0].shape)\n",
    "fig= plt.figure()\n",
    "ax= fig.gca(projection='3d')\n",
    "ax.contour(X,Y,Feature_MSC_Matrix_C2[0,0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
